# app.py â€” Streamlit ë¦¬ë·° ì˜ˆì¸¡( predict_safe.py ë™ì¹˜í™” ë²„ì „ )
# -*- coding: utf-8 -*-
from __future__ import annotations
import os, re, json
from pathlib import Path

import joblib
import numpy as np
import pandas as pd
from scipy.sparse import csr_matrix
import streamlit as st

# ---------- ê³µí†µ: ìŠ¤ë ˆë“œ/ë°±ì—”ë“œ ì œí•œ ----------
os.environ.setdefault("OMP_NUM_THREADS", "1")
os.environ.setdefault("OPENBLAS_NUM_THREADS", "1")
os.environ.setdefault("MKL_NUM_THREADS", "1")
os.environ.setdefault("NUMEXPR_NUM_THREADS", "1")

# skops ì•ˆì „ ë¡œë”
try:
    from skops.io import load as skops_load, get_untrusted_types
    HAS_SKOPS = True
except Exception:
    HAS_SKOPS = False

ALLOWED_PREFIXES = ("sklearn.", "numpy.", "scipy.", "xgboost.", "lightgbm.")

def safe_skops_load(path: Path):
    p = str(path)
    try:
        types = get_untrusted_types(file=p)
    except TypeError:
        try:
            types = get_untrusted_types(path=p)
        except TypeError:
            types = get_untrusted_types()
    bad = [t for t in types if not t.startswith(ALLOWED_PREFIXES)]
    if bad:
        raise RuntimeError(f"skops íŒŒì¼ ë¹„í—ˆìš© íƒ€ì… í¬í•¨: {bad[:5]} ...")
    return skops_load(p, trusted=types)

# ---------- RF í•˜ìœ„ ì¶”ì •ê¸° íŒ¨ì¹˜ ----------
def patch_rf_monotonic(reg):
    try:
        from sklearn.ensemble import RandomForestRegressor
        from sklearn.pipeline import Pipeline
        rf = None
        if "Pipeline" in str(type(reg)):
            last = reg.steps[-1][1]
            if isinstance(last, RandomForestRegressor):
                rf = last
        elif isinstance(reg, RandomForestRegressor):
            rf = reg
        if rf is not None and getattr(rf, "estimators_", None) is not None:
            for est in rf.estimators_:
                if not hasattr(est, "monotonic_cst"):
                    setattr(est, "monotonic_cst", None)
    except Exception:
        pass
    return reg

# ---------- í…ìŠ¤íŠ¸ ì •ì œ/í† í¬ë‚˜ì´ì¦ˆ ----------
POS_EMO = "ğŸ˜€ğŸ˜ƒğŸ˜„ğŸ˜ğŸ˜†ğŸ™‚ğŸ˜ŠğŸ˜ğŸ¤©ğŸ˜‹ğŸ˜‰ğŸ‘ğŸ™ŒğŸ‰â¤ğŸ’–ğŸ’—ğŸ’“ğŸ’ğŸ’•âœ¨ğŸ˜»ğŸ¥°ğŸ¤—ğŸ˜ºğŸ˜¸"
NEG_EMO = "ğŸ˜ğŸ˜ŸğŸ˜ ğŸ˜¡ğŸ˜¢ğŸ˜­ğŸ¤®ğŸ˜’ğŸ˜•ğŸ™â˜¹ğŸ‘ğŸ’¢ğŸ˜£ğŸ˜–ğŸ¤¬ğŸ˜¤ğŸ’”ğŸ˜¿ğŸ˜¹"
URL_RE = re.compile(r"(https?:\/\/[^\s]+)")
HTML_RE = re.compile(r"<[^>]+>")
MULTI_SPACE = re.compile(r"\s+")

def replace_emojis(text: str) -> str:
    text = re.sub(f"[{re.escape(POS_EMO)}]+", " [EMO_POS] ", text)
    text = re.sub(f"[{re.escape(NEG_EMO)}]+", " [EMO_NEG] ", text)
    text = re.sub(r"[\U00010000-\U0010ffff]", " [EMO] ", text)
    return text

def clean_text(s: str) -> str:
    if not isinstance(s, str):
        return ""
    s = s.replace("\u200b", " ")
    s = HTML_RE.sub(" ", s)
    s = URL_RE.sub(" [URL] ", s)
    s = replace_emojis(s)
    s = re.sub(r"[^0-9A-Za-zê°€-í£\.\,\!\?\[\]_ ]+", " ", s)
    s = MULTI_SPACE.sub(" ", s).strip()
    return s

def get_tokenizer():
    # 1) mecab
    try:
        from mecab import MeCab
        m = MeCab()
        def tok(text):
            return [w for (w, p) in m.pos(text)
                    if p.startswith(("NN","VV","VA","MAG","IC","XR"))]
        return "mecab_python", tok
    except Exception:
        pass
    # 2) kiwi
    try:
        from kiwipiepy import Kiwi
        kiwi = Kiwi()
        def tok(text):
            return [t.form for t in kiwi.tokenize(text)
                    if t.tag.startswith(("N","V","MAG","IC","XR","MM"))]
        return "kiwi", tok
    except Exception:
        pass
    # 3) fallback: regex
    def tok(text):
        return re.findall(r"[ê°€-í£A-Za-z0-9]{2,}", text)
    return "simple", tok

TOK_NAME, TOKENIZE = get_tokenizer()

def tokenize_and_join(text: str, stopwords:set) -> str:
    toks = TOKENIZE(clean_text(text))
    if stopwords:
        toks = [t for t in toks if t not in stopwords]
    return " ".join(toks)

# ---------- neg/pos ì—´ê°€ì¤‘ ----------
def apply_column_scaling_csc(X_csc, vocab: dict, terms: list, scale: float):
    hits = [vocab[t] for t in terms if t in vocab]
    for j in hits:
        start, end = X_csc.indptr[j], X_csc.indptr[j+1]
        if end > start:
            X_csc.data[start:end] *= scale
    return X_csc

def maybe_apply_negpos_bonus(X_csr, vec, base_dir: Path):
    summ = base_dir / "tfidf_summary.json"
    if not summ.exists():
        return X_csr
    try:
        js = json.loads(summ.read_text(encoding="utf-8"))
        info = js.get("tfidf", {})
        neg_terms = info.get("neg_terms", []) or []
        pos_terms = info.get("pos_terms", []) or []
        neg_bonus = float(info.get("neg_bonus", 1.0))
        pos_bonus = float(info.get("pos_bonus", 1.0))
        if (neg_terms or pos_terms) and hasattr(vec, "vocabulary_"):
            vocab = vec.vocabulary_
            X_csc = X_csr.tocsc(copy=True)
            if neg_terms and neg_bonus != 1.0:
                X_csc = apply_column_scaling_csc(X_csc, vocab, neg_terms, neg_bonus)
            if pos_terms and pos_bonus != 1.0:
                X_csc = apply_column_scaling_csc(X_csc, vocab, pos_terms, pos_bonus)
            return X_csc.tocsr(copy=False)
    except Exception:
        return X_csr
    return X_csr

# ---------- ìì‚° ë¡œë“œ ----------
def load_vectorizer_with_compat(base: Path, model_expected_n: int | None):
    cands = []
    sk = base / "tfidf_vectorizer.skops"
    pk = base / "tfidf_vectorizer.pkl"
    if sk.exists() and HAS_SKOPS:
        cands.append(("skops", sk))
    if pk.exists():
        cands.append(("pkl", pk))
    if not cands:
        raise FileNotFoundError("tfidf_vectorizer.(skops|pkl) ì—†ìŒ")

    last_err, chosen = None, None
    for kind, path in cands:
        try:
            vec = safe_skops_load(path) if kind == "skops" else joblib.load(path)
            n_vec = len(vec.get_feature_names_out())
            if (model_expected_n is None) or (n_vec == model_expected_n):
                chosen = vec
                break
            else:
                last_err = f"{path.name} features {n_vec} != expected {model_expected_n}"
        except Exception as e:
            last_err = f"load {path.name} failed: {e!r}"
    if chosen is None:
        raise ValueError(f"ì í•©í•œ TF-IDF ë²¡í„°ë¼ì´ì €ë¥¼ ì°¾ì§€ ëª»í•¨: {last_err}")
    return chosen

def load_assets(base_dir: Path):
    base = Path(base_dir)
    # SGD (ì„¤ëª…ìš©)
    if HAS_SKOPS and (base / "sgd_logistic_cls.skops").exists():
        cls = safe_skops_load(base / "sgd_logistic_cls.skops")
    elif (base / "sgd_logistic_cls.joblib").exists():
        cls = joblib.load(base / "sgd_logistic_cls.joblib")
    else:
        cls = None
    model_expected_n_cls = getattr(cls, "n_features_in_", None) if cls is not None else None

    # RF
    if HAS_SKOPS and (base / "rf_reg.skops").exists():
        reg = safe_skops_load(base / "rf_reg.skops")
    elif (base / "rf_reg.joblib").exists():
        reg = joblib.load(base / "rf_reg.joblib")
    else:
        raise FileNotFoundError("rf_reg.(skops|joblib) ì—†ìŒ")
    reg = patch_rf_monotonic(reg)
    model_expected_n_reg = getattr(reg, "n_features_in_", None)

    # Vectorizer: (ê°€ëŠ¥í•˜ë©´) SGD ê¸°ì¤€ìœ¼ë¡œ ë§ì¶”ë˜, RFì™€ë„ ì¼ì¹˜ í™•ì¸
    vec = load_vectorizer_with_compat(base, model_expected_n_cls or model_expected_n_reg)
    n_vec = len(vec.get_feature_names_out())
    if (model_expected_n_reg is not None) and (n_vec != model_expected_n_reg):
        raise ValueError(f"ë²¡í„°ë¼ì´ì € í”¼ì²˜ìˆ˜ {n_vec} != RF ê¸°ëŒ€ {model_expected_n_reg}")

    # ë¶ˆìš©ì–´
    stop = set()
    for sw in [base.parent / "stopwords_ko.txt", base / "stopwords_ko.txt"]:
        if sw.exists():
            stop = {x.strip() for x in sw.read_text(encoding="utf-8").splitlines() if x.strip()}
            break

    return vec, reg, cls, stop, base

# ---------- ìœ„í—˜ë„ ----------
def risk_level(avg_score: float) -> str:
    if avg_score >= 4.10: return "Safe"
    if avg_score >= 4.00: return "Low"
    if avg_score >= 3.90: return "Medium"
    return "High"

def risk_color(level: str) -> str:
    return {"Safe":"#2e7d32","Low":"#558b2f","Medium":"#f9a825","High":"#c62828"}.get(level, "#333")

# ---------- ë¶€ì •ì  í† í°(ìƒìœ„3) : (coef5 - coef1) * tfidf ê°€ì¥ ìŒìˆ˜ ----------
def negative_top_terms(vec, cls, Xrow: csr_matrix, topk=3):
    if cls is None or not hasattr(cls, "coef_"):  # SGD ì—†ìœ¼ë©´ ë¶ˆê°€
        return []
    feats = vec.get_feature_names_out()
    coef = cls.coef_       # shape: [n_classes, n_features]
    classes = getattr(cls, "classes_", np.arange(coef.shape[0]))
    # class 1ê³¼ class 5 ì¸ë±ìŠ¤ ì°¾ê¸°
    def find_idx(c):
        idx = np.where(classes == c)[0]
        return int(idx[0]) if len(idx) else None
    i1, i5 = find_idx(1), find_idx(5)
    if i1 is None or i5 is None:
        return []
    delta = coef[i5] - coef[i1]   # ì–‘ìˆ˜ë©´ ë³„5 ìª½, ìŒìˆ˜ë©´ ë³„1 ìª½
    x = Xrow.toarray().ravel()
    contrib = delta * x
    # ìŒìˆ˜(í•˜ë½) ê¸°ì—¬ê°€ í° ìˆœì„œ
    idx = np.argsort(contrib)[:topk]
    rows = []
    for j in idx:
        if x[j] == 0: 
            continue
        rows.append({"term": str(feats[j]), "tfidf": float(x[j]), "coef_delta": float(delta[j]), "score_pull": float(contrib[j])})
    return rows

# ---------- Streamlit UI ----------
st.set_page_config(page_title="ë¦¬ë·° ì˜ˆì¸¡/ë¶„ì„", page_icon="â­", layout="wide")
ROOT = Path(__file__).resolve().parent
MODELS = ROOT / "models"

st.title("â­ ë¦¬ë·° ì˜ˆì¸¡ ë°ëª¨ (safe-ë™ì¼í™”)")

# ê²½ë¡œ ì…ë ¥(ê³ ì • ê²½ë¡œ ì“°ë©´ ìˆ˜ì •)
base_dir = MODELS
vec, reg, cls, stop, base = load_assets(base_dir)

with st.expander("ë””ë²„ê·¸(ëª¨ë¸/ë²¡í„°ë¼ì´ì € ì¼ì¹˜ í™•ì¸)"):
    st.write({
        "tokenizer": TOK_NAME,
        "stopwords": len(stop),
        "vectorizer_features": len(vec.get_feature_names_out()),
        "rf_n_features_in_": getattr(reg, "n_features_in_", None),
        "sgd_n_features_in_": getattr(cls, "n_features_in_", None) if cls is not None else None,
        "negpos_summary": (base / "tfidf_summary.json").exists()
    })

sgd_explain = st.toggle("SGD ê¸°ë°˜ ì„¤ëª…(ë¶€ì • í† í°) í‘œì‹œ", value=True, help="SGDê°€ ìˆì„ ë•Œë§Œ í‘œì‹œ")

# ë‹¨ì¼ ì˜ˆì¸¡
st.subheader("ë‹¨ì¼ í…ìŠ¤íŠ¸ ì˜ˆì¸¡")
inp = st.text_area("ë¦¬ë·° í…ìŠ¤íŠ¸ ì…ë ¥", height=160, placeholder="ë¦¬ë·°ë¥¼ ë¶™ì—¬ë„£ìœ¼ì„¸ìš”â€¦")
if st.button("ì˜ˆì¸¡í•˜ê¸°") and inp.strip():
    toks = tokenize_and_join(inp, stop)
    X = vec.transform([toks])
    X = maybe_apply_negpos_bonus(X, vec, base)
    score = float(np.clip(reg.predict(X)[0], 1, 5))
    st.metric("ì˜ˆì¸¡ ì ìˆ˜", f"{score:.2f} â˜…")

st.divider()

# ë°°ì¹˜ ì˜ˆì¸¡
st.subheader("ë°°ì¹˜ ì˜ˆì¸¡ (CSV ì—…ë¡œë“œ)")
csv = st.file_uploader("CSV ì—…ë¡œë“œ (í•„ìˆ˜ ì»¬ëŸ¼: review_text)", type=["csv"])

if csv is not None:
    try:
        df = pd.read_csv(csv)
    except Exception as e:
        st.error(f"CSV ë¡œë”© ì‹¤íŒ¨: {e}")
        st.stop()

    if "review_text" not in df.columns:
        st.error("CSVì— 'review_text' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        st.stop()

    texts = df["review_text"].fillna("").astype(str).tolist()
    toks = [tokenize_and_join(t, stop) for t in texts]
    X = vec.transform(toks)
    X = maybe_apply_negpos_bonus(X, vec, base)

    pred = np.clip(reg.predict(X), 1, 5)
    df_out = df.copy()
    df_out["pred_score"] = np.round(pred, 2)

    # í™”ë©´ìš© í‘œ
    view_cols = [c for c in ("review_text","review_date") if c in df_out.columns] + ["pred_score"]
    st.dataframe(df_out.loc[:, view_cols].rename(columns={"review_text":"ë¦¬ë·°","review_date":"ë‚ ì§œ","pred_score":"ì˜ˆì¸¡ ë³„ì "}), use_container_width=True)

    # í‰ê· /ìœ„í—˜ë„
    avg = float(df_out["pred_score"].mean())
    level = risk_level(avg)
    c1,c2 = st.columns([1,1])
    with c1:
        st.metric("í‰ê·  í‰ì ", f"{avg:.2f} â˜…")
    with c2:
        st.markdown(f"""
        <div style="padding:10px 12px;border-radius:10px;background:{risk_color(level)};color:#fff;display:inline-block;font-weight:600;">
            ìœ„í—˜ë„: {level}
        </div>""", unsafe_allow_html=True)

    # ë¶€ì • í† í° TOP3 (High/Medium && SGD on)
    if sgd_explain and level in {"High","Medium"} and cls is not None:
        # ì „ì²´ ë¬¸ì„œì˜ ë¶€ì • ê¸°ì—¬ í•©ì„ í† í°ë³„ë¡œ ëˆ„ì í•˜ì—¬ TOP3
        feats = vec.get_feature_names_out()
        agg = {}
        # (coef5 - coef1) * tfidf ìŒìˆ˜ ë°©í–¥ í•©
        coef = cls.coef_
        classes = getattr(cls, "classes_", np.arange(coef.shape[0]))
        i1 = np.where(classes == 1)[0]
        i5 = np.where(classes == 5)[0]
        if len(i1) and len(i5):
            delta = coef[int(i5[0])] - coef[int(i1[0])]
            for i in range(X.shape[0]):
                row = X[i].toarray().ravel()
                contrib = delta * row
                # ìŒìˆ˜(í•˜ë½)ë§Œ ëˆ„ì 
                neg_idx = np.where(contrib < 0)[0]
                for j in neg_idx:
                    if row[j] == 0: 
                        continue
                    agg[j] = agg.get(j, 0.0) + contrib[j]
            if agg:
                worst = sorted(agg.items(), key=lambda kv: kv[1])[:3]
                labels = [f"{feats[j]} (Î£ {aggv:.2f})" for j, aggv in worst]
                st.markdown(f"**ë¦¬ë·° ì† ë¶€ì •ì  ë‹¨ì–´ TOP3:** " + ", ".join(labels))

    # ë‹¤ìš´ë¡œë“œ
    st.download_button(
        "ê²°ê³¼ CSV ë‹¤ìš´ë¡œë“œ",
        df_out.to_csv(index=False, encoding="utf-8-sig"),
        file_name="predictions.csv",
        mime="text/csv",
    )
