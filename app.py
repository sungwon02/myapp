# app.py â€” Streamlit ë¦¬ë·° ì˜ˆì¸¡(íšŒê·€ ì „ìš©, Kiwi í† í¬ë‚˜ì´ì € ê³ ì • + neg/pos ì—´ê°€ì¤‘ ë™ê¸°í™”)
# -*- coding: utf-8 -*-

from __future__ import annotations
import json, re
from pathlib import Path

import joblib
import numpy as np
import pandas as pd
import streamlit as st
from scipy.sparse import csr_matrix

# =========================
# ê¸°ë³¸ ì„¤ì •
# =========================
st.set_page_config(page_title="ë¦¬ë·° ì˜ˆì¸¡/ë¶„ì„", page_icon="â­", layout="wide")

ROOT = Path(__file__).resolve().parent
MODELS = ROOT / "models"

# ë²¡í„°/ëª¨ë¸ ê²½ë¡œ (pkl/skops ëª¨ë‘ ì§€ì›)
VEC_PKL   = MODELS / "tfidf_vectorizer.pkl"
VEC_SKOPS = MODELS / "tfidf_vectorizer.skops"
REG_JOB   = MODELS / "rf_reg.joblib"
REG_SKOPS = MODELS / "rf_reg.skops"
SUMMARY   = MODELS / "tfidf_summary.json"

# (ì„ íƒ) skops ë¡œë”
ALLOWED_PREFIXES = ("sklearn.", "numpy.", "scipy.", "xgboost.", "lightgbm.")
try:
    from skops.io import load as skops_load, get_untrusted_types
    HAS_SKOPS = True
except Exception:
    HAS_SKOPS = False

def _assert_files_exist(paths):
    miss = [p for p in paths if not p.exists()]
    if miss:
        st.error(f"ëª¨ë¸ íŒŒì¼ ëˆ„ë½: {[str(p) for p in miss]}")
        st.stop()

def _safe_skops_load(path: Path):
    """skops 0.10+ ì‹ ë¢° íƒ€ì… ì ê²€ í›„ ë¡œë“œ"""
    p = str(path)
    types = None
    from typing import Any
    try:
        types = get_untrusted_types(file=p)  # skops>=0.10
    except TypeError:
        try:
            types = get_untrusted_types(path=p)  # ì¼ë¶€ ë²„ì „ í˜¸í™˜
        except TypeError:
            types = get_untrusted_types()
    bad = [t for t in types if not t.startswith(ALLOWED_PREFIXES)]
    if bad:
        raise RuntimeError(
            "skops íŒŒì¼ì— ë¹„í—ˆìš© íƒ€ì…ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n"
            f"- íŒŒì¼: {p}\n- ë¹„í—ˆìš© ì˜ˆ: {bad[:5]} ..."
        )
    return skops_load(p, trusted=types)

def _patch_rf_monotonic(reg_model):
    """RandomForestRegressor í•˜ìœ„ íŠ¸ë¦¬ì˜ monotonic_cst ì†ì„± ëˆ„ë½ ë³´ì •"""
    try:
        from sklearn.ensemble import RandomForestRegressor
        from sklearn.pipeline import Pipeline
        rf = None
        if isinstance(reg_model, Pipeline):
            last = reg_model.steps[-1][1]
            if isinstance(last, RandomForestRegressor):
                rf = last
        elif isinstance(reg_model, RandomForestRegressor):
            rf = reg_model
        if rf is not None and getattr(rf, "estimators_", None) is not None:
            for est in rf.estimators_:
                if not hasattr(est, "monotonic_cst"):
                    setattr(est, "monotonic_cst", None)
    except Exception:
        pass
    return reg_model

# =========================
# ì „ì²˜ë¦¬/í† í¬ë‚˜ì´ì¦ˆ (Kiwi ê³ ì •)
# =========================
POS_EMO = "ğŸ˜€ğŸ˜ƒğŸ˜„ğŸ˜ğŸ˜†ğŸ™‚ğŸ˜ŠğŸ˜ğŸ¤©ğŸ˜‹ğŸ˜‰ğŸ‘ğŸ™ŒğŸ‰â¤ğŸ’–ğŸ’—ğŸ’“ğŸ’ğŸ’•âœ¨ğŸ˜»ğŸ¥°ğŸ¤—ğŸ˜ºğŸ˜¸"
NEG_EMO = "ğŸ˜ğŸ˜ŸğŸ˜ ğŸ˜¡ğŸ˜¢ğŸ˜­ğŸ¤®ğŸ˜’ğŸ˜•ğŸ™â˜¹ğŸ‘ğŸ’¢ğŸ˜£ğŸ˜–ğŸ¤¬ğŸ˜¤ğŸ’”ğŸ˜¿ğŸ˜¹"
URL_RE = re.compile(r"(https?:\/\/[^\s]+)")
HTML_RE = re.compile(r"<[^>]+>")
MULTI_SPACE = re.compile(r"\s+")

def _replace_emojis(text: str) -> str:
    text = re.sub(f"[{re.escape(POS_EMO)}]+", " [EMO_POS] ", text)
    text = re.sub(f"[{re.escape(NEG_EMO)}]+", " [EMO_NEG] ", text)
    text = re.sub(r"[\U00010000-\U0010ffff]", " [EMO] ", text)
    return text

def _clean_text(s: str) -> str:
    if not isinstance(s, str):
        return ""
    s = s.replace("\u200b", " ")
    s = HTML_RE.sub(" ", s)
    s = URL_RE.sub(" [URL] ", s)
    s = _replace_emojis(s)
    s = re.sub(r"[^0-9A-Za-zê°€-í£\.\,\!\?\[\]_ ]+", " ", s)
    s = MULTI_SPACE.sub(" ", s).strip()
    return s

# ---- Kiwi í† í¬ë‚˜ì´ì € ê³ ì • (predict_safe.pyì™€ ë™ì¼ í’ˆì‚¬êµ°) ----
try:
    from kiwipiepy import Kiwi
    _KIWI = Kiwi()
    _TOKENIZER_NAME = "kiwi"
except Exception as e:
    raise RuntimeError(
        "Kiwi í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. "
        "requirements.txtì— 'kiwipiepy'ë¥¼ ì¶”ê°€/ì„¤ì¹˜í•˜ê³  ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”."
    ) from e

def _tokenize(text: str) -> list[str]:
    return [t.form for t in _KIWI.tokenize(text)
            if t.tag.startswith(("N","V","MAG","IC","XR","MM"))]

def tokenize_and_join(s: str, stopwords: set[str] | None = None) -> str:
    toks = _tokenize(_clean_text(s))
    if stopwords:
        toks = [t for t in toks if t not in stopwords]
    return " ".join(toks)

# =========================
# neg/pos ì—´ê°€ì¤‘ (predict_safe.pyì™€ ë™ì¼)
# =========================
def _apply_column_scaling_csc(X_csc, vocab: dict, terms: list, scale: float):
    hits = [vocab[t] for t in terms if t in vocab]
    for j in hits:
        start, end = X_csc.indptr[j], X_csc.indptr[j+1]
        if end > start:
            X_csc.data[start:end] *= scale
    return X_csc

def maybe_apply_negpos_bonus(X_csr, vec, base_dir: Path):
    summ = base_dir / "tfidf_summary.json"
    if not summ.exists():
        return X_csr
    try:
        with open(summ, "r", encoding="utf-8") as f:
            js = json.load(f)
        info = js.get("tfidf", {})
        neg_terms = info.get("neg_terms", []) or []
        pos_terms = info.get("pos_terms", []) or []
        neg_bonus = float(info.get("neg_bonus", 1.0))
        pos_bonus = float(info.get("pos_bonus", 1.0))
        if (neg_terms or pos_terms) and hasattr(vec, "vocabulary_"):
            vocab = vec.vocabulary_
            X_csc = X_csr.tocsc(copy=True)
            if neg_terms and neg_bonus != 1.0:
                X_csc = _apply_column_scaling_csc(X_csc, vocab, neg_terms, neg_bonus)
            if pos_terms and pos_bonus != 1.0:
                X_csc = _apply_column_scaling_csc(X_csc, vocab, pos_terms, pos_bonus)
            return X_csc.tocsr(copy=False)
    except Exception:
        return X_csr
    return X_csr

# =========================
# ëª¨ë¸ ë¡œë“œ (ìºì‹œ)
# =========================
@st.cache_resource(show_spinner=True)
def load_models():
    # ë²¡í„°ë¼ì´ì €
    if VEC_SKOPS.exists() and HAS_SKOPS:
        vec = _safe_skops_load(VEC_SKOPS)
    else:
        _assert_files_exist([VEC_PKL])
        vec = joblib.load(VEC_PKL)

    # RF íšŒê·€
    if REG_SKOPS.exists() and HAS_SKOPS:
        reg = _safe_skops_load(REG_SKOPS)
    else:
        _assert_files_exist([REG_JOB])
        reg = joblib.load(REG_JOB)
    reg = _patch_rf_monotonic(reg)

    # (ì„ íƒ) stopwords
    stop = set()
    for sw in [ROOT.parent / "stopwords_ko.txt", ROOT / "stopwords_ko.txt", MODELS / "stopwords_ko.txt"]:
        if Path(sw).exists():
            with open(sw, encoding="utf-8") as f:
                stop = {x.strip() for x in f if x.strip()}
            break

    # ë””ë²„ê·¸ìš© ë©”íƒ€
    dbg = {
        "tokenizer": _TOKENIZER_NAME,
        "stopwords": len(stop),
        "vectorizer_features": len(vec.get_feature_names_out()) if hasattr(vec, "get_feature_names_out") else None,
        "rf_n_features_in_": getattr(reg, "n_features_in_", None),
        "negpos_summary": SUMMARY.exists(),
    }
    return vec, reg, stop, dbg

# =========================
# ìœ„í—˜ë„ íŒì •
# =========================
def risk_level(avg_score: float) -> str:
    # Safe â‰¥ 4.10, Low â‰¥ 4.00, Medium â‰¥ 3.90, High < 3.90
    if avg_score >= 4.10:
        return "Safe"
    if avg_score >= 4.00:
        return "Low"
    if avg_score >= 3.90:
        return "Medium"
    return "High"

def risk_color(level: str) -> str:
    return {
        "Safe":   "#2e7d32",
        "Low":    "#558b2f",
        "Medium": "#f9a825",
        "High":   "#c62828",
    }.get(level, "#333333")

# ==========================================================
#                          UI
# ==========================================================
st.title("â­ ë¦¬ë·° ì˜ˆì¸¡ ë°ëª¨ (RF íšŒê·€ + Kiwi)")

vec, reg, stopwords, dbg = load_models()

with st.expander("ë””ë²„ê·¸(ëª¨ë¸/ë²¡í„°ë¼ì´ì € ì¼ì¹˜ í™•ì¸)"):
    st.json(dbg)

# â”€â”€ ë‹¨ì¼ ì˜ˆì¸¡
st.subheader("ë‹¨ì¼ í…ìŠ¤íŠ¸ ì˜ˆì¸¡")
inp = st.text_area("ë¦¬ë·° í…ìŠ¤íŠ¸ ì…ë ¥", height=160, placeholder="ë¦¬ë·°ë¥¼ ë¶™ì—¬ë„£ìœ¼ì„¸ìš”â€¦")
if st.button("ì˜ˆì¸¡í•˜ê¸°") and inp.strip():
    toks = tokenize_and_join(inp, stopwords)
    X = vec.transform([toks])
    X = maybe_apply_negpos_bonus(X, vec, MODELS)
    score = float(np.clip(reg.predict(X)[0], 1, 5))  # 1~5ë¡œ í´ë¦½
    st.metric("ì˜ˆì¸¡ ì ìˆ˜", f"{score:.2f} â˜…")

st.divider()

# â”€â”€ ë°°ì¹˜ ì˜ˆì¸¡
st.subheader("ë°°ì¹˜ ì˜ˆì¸¡ (CSV ì—…ë¡œë“œ)")
csv = st.file_uploader("CSV ì—…ë¡œë“œ (í•„ìˆ˜ ì»¬ëŸ¼: review_text)", type=["csv"])

if csv is not None:
    try:
        df = pd.read_csv(csv)
    except Exception as e:
        st.error(f"CSV ë¡œë”© ì‹¤íŒ¨: {e}")
    else:
        if "review_text" not in df.columns:
            st.error("CSVì— 'review_text' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        else:
            # ë²¡í„°í™”/ì˜ˆì¸¡
            toks = df["review_text"].fillna("").astype(str).map(lambda s: tokenize_and_join(s, stopwords))
            X = vec.transform(toks)
            Xb = maybe_apply_negpos_bonus(X, vec, MODELS)
            df["pred_score"] = np.clip(reg.predict(Xb), 1, 5).round(2)

            # í™”ë©´ í‘œì‹œìš© ì»¬ëŸ¼ êµ¬ì„± (queryëŠ” ìˆ¨ê¹€)
            view_cols = []
            if "review_text" in df.columns:
                view_cols.append("review_text")
            if "review_date" in df.columns:
                view_cols.append("review_date")
            view_cols.append("pred_score")

            df_view = df.loc[:, view_cols].rename(
                columns={
                    "review_text": "ë¦¬ë·°",
                    "review_date": "ë‚ ì§œ",
                    "pred_score":  "ì˜ˆì¸¡ ë³„ì ",
                }
            )

            st.dataframe(df_view, use_container_width=True)

            # ===== í‰ê·  & ìœ„í—˜ë„ =====
            avg = float(df["pred_score"].mean())
            level = risk_level(avg)
            col1, col2 = st.columns([1, 1])
            with col1:
                st.metric("RF í‰ê·  í‰ì ", f"{avg:.2f} â˜…")
            with col2:
                st.markdown(
                    f"""
                    <div style="padding:10px 12px;border-radius:10px;
                                background:{risk_color(level)};color:#fff;
                                display:inline-block;font-weight:600;">
                        ìœ„í—˜ë„: {level}
                    </div>
                    """,
                    unsafe_allow_html=True,
                )

            # ===== ë‹¤ìš´ë¡œë“œ =====
            st.download_button(
                "ê²°ê³¼ CSV ë‹¤ìš´ë¡œë“œ",
                df.to_csv(index=False, encoding="utf-8-sig"),
                file_name="predictions.csv",
                mime="text/csv",
            )
