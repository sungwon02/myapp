# app.py â€” Streamlit ë¦¬ë·° ì˜ˆì¸¡ (predict_safe.py ë°©ì‹ ë°˜ì˜)
# -*- coding: utf-8 -*-

from __future__ import annotations
import os, re, json
from pathlib import Path

import joblib
import numpy as np
import pandas as pd
import streamlit as st
from scipy.sparse import csr_matrix

# =========================
# ê¸°ë³¸ ì„¤ì •
# =========================
st.set_page_config(page_title="ë¦¬ë·° ì˜ˆì¸¡/ë¶„ì„", page_icon="â­", layout="wide")

ROOT = Path(__file__).resolve().parent
MODELS = ROOT / "models"

# íŒŒì¼ ê²½ë¡œ(ìš°ì„  skops, ì—†ìœ¼ë©´ pkl/joblib)
VEC_SKOPS = MODELS / "tfidf_vectorizer.skops"
VEC_PKL   = MODELS / "tfidf_vectorizer.pkl"
SGD_SKOPS = MODELS / "sgd_logistic_cls.skops"
SGD_JOB   = MODELS / "sgd_logistic_cls.joblib"
RF_SKOPS  = MODELS / "rf_reg.skops"
RF_JOB    = MODELS / "rf_reg.joblib"
SUMMARY   = MODELS / "tfidf_summary.json"

# ============== skops ì•ˆì „ ë¡œë” ==============
try:
    from skops.io import load as skops_load, get_untrusted_types
    HAS_SKOPS = True
except Exception:
    HAS_SKOPS = False

ALLOWED_PREFIXES = ("sklearn.", "numpy.", "scipy.", "xgboost.", "lightgbm.")

def safe_skops_load(path: Path):
    p = str(path)
    types = None
    if HAS_SKOPS:
        try:
            # skops 0.10+ ì‹œê·¸ë‹ˆì²˜ í˜¸í™˜
            try:
                types = get_untrusted_types(file=p)
            except TypeError:
                types = get_untrusted_types(path=p)
        except TypeError:
            types = get_untrusted_types()
    bad = [t for t in (types or []) if not t.startswith(ALLOWED_PREFIXES)]
    if bad:
        raise RuntimeError(
            "skops íŒŒì¼ì— ë¹„í—ˆìš© íƒ€ì…ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n"
            f"- íŒŒì¼: {p}\n- ë¹„í—ˆìš© ì˜ˆ: {bad[:5]} ..."
        )
    return skops_load(p, trusted=types) if HAS_SKOPS else None

# ============== RF í˜¸í™˜ íŒ¨ì¹˜ ==============
def _patch_rf_monotonic(reg_model):
    try:
        from sklearn.ensemble import RandomForestRegressor
        from sklearn.pipeline import Pipeline
        rf = None
        if isinstance(reg_model, Pipeline):
            last = reg_model.steps[-1][1]
            if isinstance(last, RandomForestRegressor):
                rf = last
        elif isinstance(reg_model, RandomForestRegressor):
            rf = reg_model
        if rf is not None and getattr(rf, "estimators_", None) is not None:
            for est in rf.estimators_:
                if not hasattr(est, "monotonic_cst"):
                    setattr(est, "monotonic_cst", None)
    except Exception:
        pass
    return reg_model

# =========================
# ì „ì²˜ë¦¬/í† í¬ë‚˜ì´ì¦ˆ (predict_safeì™€ ë™ì¼)
# =========================
POS_EMO = "ğŸ˜€ğŸ˜ƒğŸ˜„ğŸ˜ğŸ˜†ğŸ™‚ğŸ˜ŠğŸ˜ğŸ¤©ğŸ˜‹ğŸ˜‰ğŸ‘ğŸ™ŒğŸ‰â¤ğŸ’–ğŸ’—ğŸ’“ğŸ’ğŸ’•âœ¨ğŸ˜»ğŸ¥°ğŸ¤—ğŸ˜ºğŸ˜¸"
NEG_EMO = "ğŸ˜ğŸ˜ŸğŸ˜ ğŸ˜¡ğŸ˜¢ğŸ˜­ğŸ¤®ğŸ˜’ğŸ˜•ğŸ™â˜¹ğŸ‘ğŸ’¢ğŸ˜£ğŸ˜–ğŸ¤¬ğŸ˜¤ğŸ’”ğŸ˜¿ğŸ˜¹"
URL_RE = re.compile(r"(https?:\/\/[^\s]+)")
HTML_RE = re.compile(r"<[^>]+>")
MULTI_SPACE = re.compile(r"\s+")

def _replace_emojis(text: str) -> str:
    text = re.sub(f"[{re.escape(POS_EMO)}]+", " [EMO_POS] ", text)
    text = re.sub(f"[{re.escape(NEG_EMO)}]+", " [EMO_NEG] ", text)
    text = re.sub(r"[\U00010000-\U0010ffff]", " [EMO] ", text)
    return text

def _clean_text(s: str) -> str:
    if not isinstance(s, str):
        return ""
    s = s.replace("\u200b", " ")
    s = HTML_RE.sub(" ", s)
    s = URL_RE.sub(" [URL] ", s)
    s = _replace_emojis(s)
    s = re.sub(r"[^0-9A-Za-zê°€-í£\.\,\!\?\[\]_ ]+", " ", s)
    s = MULTI_SPACE.sub(" ", s).strip()
    return s

def get_tokenizer():
    # ê°„ê²°í™”: regex ê¸°ë³¸, mecab/kiwi ìˆìœ¼ë©´ ìë™ ì‚¬ìš©
    try:
        from mecab import MeCab
        m = MeCab()
        def tok(text):
            return [w for (w, p) in m.pos(text)
                    if p.startswith(("NN","VV","VA","MAG","IC","XR"))]
        return "mecab_python", tok
    except Exception:
        pass
    try:
        from kiwipiepy import Kiwi
        kiwi = Kiwi()
        def tok(text):
            return [t.form for t in kiwi.tokenize(text)
                    if t.tag.startswith(("N","V","MAG","IC","XR","MM"))]
        return "kiwi", tok
    except Exception:
        pass
    def tok(text):
        return re.findall(r"[ê°€-í£A-Za-z0-9]{2,}", text)
    return "simple", tok

TOK_NAME, TOKENIZE = get_tokenizer()

def tokenize_and_join(text: str, stopwords:set) -> str:
    toks = TOKENIZE(_clean_text(text))
    if stopwords:
        toks = [t for t in toks if t not in stopwords]
    return " ".join(toks)

# =========================
# neg/pos ì—´ê°€ì¤‘ (predict_safeì™€ ë™ì¼)
# =========================
def apply_column_scaling_csc(X_csc, vocab: dict, terms: list, scale: float):
    hits = [vocab[t] for t in terms if t in vocab]
    for j in hits:
        start, end = X_csc.indptr[j], X_csc.indptr[j+1]
        if end > start:
            X_csc.data[start:end] *= scale
    return X_csc

def maybe_apply_negpos_bonus(X_csr, vec, base_dir: Path):
    summ = base_dir / "tfidf_summary.json"
    if not summ.exists():
        return X_csr
    try:
        js = json.loads(summ.read_text(encoding="utf-8"))
        info = js.get("tfidf", {})
        neg_terms = info.get("neg_terms", []) or []
        pos_terms = info.get("pos_terms", []) or []
        neg_bonus = float(info.get("neg_bonus", 1.0))
        pos_bonus = float(info.get("pos_bonus", 1.0))
        if (neg_terms or pos_terms) and hasattr(vec, "vocabulary_"):
            vocab = vec.vocabulary_
            X_csc = X_csr.tocsc(copy=True)
            if neg_terms and neg_bonus != 1.0:
                X_csc = apply_column_scaling_csc(X_csc, vocab, neg_terms, neg_bonus)
            if pos_terms and pos_bonus != 1.0:
                X_csc = apply_column_scaling_csc(X_csc, vocab, pos_terms, pos_bonus)
            return X_csc.tocsr(copy=False)
    except Exception:
        return X_csr
    return X_csr

# =========================
# ëª¨ë¸ ë¡œë“œ (predict_safe ê·œì¹™ ë°˜ì˜)
# =========================
def _load_vectorizer(model_expected_n: int | None):
    # skops ìš°ì„ , ë¶ˆì¼ì¹˜ ì‹œ pkl ì‹œë„
    candidates = []
    if VEC_SKOPS.exists() and HAS_SKOPS:
        candidates.append(("skops", VEC_SKOPS))
    if VEC_PKL.exists():
        candidates.append(("pkl", VEC_PKL))
    if not candidates:
        st.error("tfidf_vectorizer.(skops|pkl) ê°€ ì—†ìŠµë‹ˆë‹¤."); st.stop()

    last_err = None
    for kind, path in candidates:
        try:
            vec = safe_skops_load(path) if kind == "skops" else joblib.load(path)
            n_vec = len(vec.get_feature_names_out())
            if (model_expected_n is None) or (n_vec == model_expected_n):
                return vec
            else:
                last_err = (f"{path.name} features={n_vec}, "
                            f"but model expects {model_expected_n}")
        except Exception as e:
            last_err = f"{path.name} load failed: {e!r}"
    st.error(f"ì í•©í•œ TF-IDF ë²¡í„°ë¼ì´ì €ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë§ˆì§€ë§‰ ì˜¤ë¥˜: {last_err}")
    st.stop()

def _load_sgd():
    if HAS_SKOPS and SGD_SKOPS.exists():
        return safe_skops_load(SGD_SKOPS)
    if SGD_JOB.exists():
        return joblib.load(SGD_JOB)
    return None  # SGD ì„ íƒì 

def _load_rf():
    if HAS_SKOPS and RF_SKOPS.exists():
        return _patch_rf_monotonic(safe_skops_load(RF_SKOPS))
    if RF_JOB.exists():
        return _patch_rf_monotonic(joblib.load(RF_JOB))
    st.error("rf_reg.(skops|joblib) ì´ ì—†ìŠµë‹ˆë‹¤."); st.stop()

@st.cache_resource(show_spinner=True)
def load_assets():
    # stopwords: ìƒìœ„/ë™ì¼ í´ë” íƒìƒ‰
    stop = set()
    for sw in [ROOT / "stopwords_ko.txt", MODELS / "stopwords_ko.txt"]:
        if sw.exists():
            stop = {x.strip() for x in sw.read_text(encoding="utf-8").splitlines() if x.strip()}
            break

    sgd = _load_sgd()  # ì—†ìœ¼ë©´ None
    rf  = _load_rf()

    # ë²¡í„°ë¼ì´ì €ëŠ” "SGD ê¸°ì¤€"ìœ¼ë¡œ ë§ì¶”ê³ , RFì™€ë„ ì¼ì¹˜ í™•ì¸
    model_expected_n_cls = getattr(sgd, "n_features_in_", None) if sgd is not None else None
    vec = _load_vectorizer(model_expected_n_cls)

    n_vec = len(vec.get_feature_names_out())
    model_expected_n_reg = getattr(rf, "n_features_in_", None)
    if (model_expected_n_reg is not None) and (n_vec != model_expected_n_reg):
        st.error(
            f"[íŠ¹ì§• ë¶ˆì¼ì¹˜] vectorizer={n_vec}, rf_reg expects={model_expected_n_reg}\n"
            "â†’ í•™ìŠµ ì‹œ ë™ì¼ ë²¡í„°ë¼ì´ì €ë¡œ í›ˆë ¨í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”."
        )
        st.stop()

    return vec, sgd, rf, stop

# =========================
# SGD ì„¤ëª…(íƒ‘í…€) ë„ìš°ë¯¸
# =========================
def explain_top_terms(vec, cls, Xrow: csr_matrix, topk=8):
    feats = vec.get_feature_names_out()
    try:
        proba = cls.predict_proba(Xrow)[0]
    except Exception:
        proba = None
    y_label = cls.predict(Xrow)[0]
    classes = getattr(cls, "classes_", None)
    if classes is None:
        y_idx = int(y_label) - 1
    else:
        idx_arr = np.where(classes == y_label)[0]
        y_idx = int(idx_arr[0]) if len(idx_arr) else int(y_label) - 1
    if not hasattr(cls, "coef_"):
        return int(y_label), (proba.tolist() if proba is not None else None), []
    coef = cls.coef_[y_idx]
    x = Xrow.toarray().ravel()
    contrib = coef * x
    idx = np.argsort(contrib)[::-1]
    rows = []
    feats_arr = np.asarray(feats)
    for j in idx:
        if x[j] == 0:
            continue
        rows.append({
            "term": str(feats_arr[j]),
            "tfidf": float(x[j]),
            "coef": float(coef[j]),
            "contrib": float(contrib[j])
        })
        if len(rows) >= topk:
            break
    return int(y_label), (proba.tolist() if proba is not None else None), rows

# =========================
# ìœ„í—˜ë„(ì´ì „ UI ìœ ì§€)
# =========================
def risk_level(avg_score: float) -> str:
    if avg_score >= 4.10: return "Safe"
    if avg_score >= 4.00: return "Low"
    if avg_score >= 3.90: return "Medium"
    return "High"

def risk_color(level: str) -> str:
    return {"Safe":"#2e7d32","Low":"#558b2f","Medium":"#f9a825","High":"#c62828"}.get(level,"#333")

# ==========================================================
#                          UI
# ==========================================================
st.title("â­ ë¦¬ë·° ì˜ˆì¸¡ ë°ëª¨ (predict_safe ë™ê¸°í™”)")

vec, sgd, reg, stop = load_assets()
st.caption(f"Tokenizer = **{TOK_NAME}**, Stopwords = **{len(stop)}**ê°œ, "
           f"ë²¡í„° íŠ¹ì„±ìˆ˜ = **{len(vec.get_feature_names_out())}**")

show_sgd = st.toggle("SGD ë¶„ë¥˜ ê²°ê³¼/ì„¤ëª…ë„ í•¨ê»˜ ë³´ê¸°", value=False)

# â”€â”€ ë‹¨ì¼ ì˜ˆì¸¡
st.subheader("ë‹¨ì¼ í…ìŠ¤íŠ¸ ì˜ˆì¸¡")
inp = st.text_area("ë¦¬ë·° í…ìŠ¤íŠ¸ ì…ë ¥", height=160, placeholder="ë¦¬ë·°ë¥¼ ë¶™ì—¬ë„£ìœ¼ì„¸ìš”â€¦")
if st.button("ì˜ˆì¸¡í•˜ê¸°") and inp.strip():
    toks = tokenize_and_join(inp, stop)
    X = vec.transform([toks])

    # ì—´ê°€ì¤‘(neg/pos) ë™ê¸°í™”
    X2 = maybe_apply_negpos_bonus(X, vec, MODELS)

    # RF íšŒê·€
    score = float(np.clip(reg.predict(X2)[0], 1, 5))
    st.metric("RF ì˜ˆì¸¡ ì ìˆ˜", f"{score:.2f} â˜…")

    # SGD (ì˜µì…˜)
    if show_sgd and sgd is not None:
        y, proba, terms = explain_top_terms(vec, sgd, X2, topk=8)
        st.write(f"**SGD ì˜ˆì¸¡ í´ë˜ìŠ¤(ë³„ì )**: {y}")
        if proba is not None:
            st.write("**í™•ë¥ **:", np.round(proba, 3))
        st.write("**Top terms (coefÂ·contrib)**:")
        st.json(terms)

st.divider()

# â”€â”€ ë°°ì¹˜ ì˜ˆì¸¡
st.subheader("ë°°ì¹˜ ì˜ˆì¸¡ (CSV ì—…ë¡œë“œ)")
csv = st.file_uploader("CSV ì—…ë¡œë“œ (í•„ìˆ˜ ì»¬ëŸ¼: review_text)", type=["csv"])

if csv is not None:
    try:
        df = pd.read_csv(csv)
    except Exception as e:
        st.error(f"CSV ë¡œë”© ì‹¤íŒ¨: {e}")
    else:
        if "review_text" not in df.columns:
            st.error("CSVì— 'review_text' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        else:
            toks = df["review_text"].fillna("").astype(str).map(lambda s: tokenize_and_join(s, stop))
            X = vec.transform(toks)
            X2 = maybe_apply_negpos_bonus(X, vec, MODELS)

            # RF íšŒê·€
            df["pred_star_reg"] = np.clip(reg.predict(X2), 1, 5).round(2)

            if show_sgd and sgd is not None:
                # SGD ë¶„ë¥˜ + í™•ë¥  + ì„¤ëª…
                cls_pred = sgd.predict(X2)
                df["pred_star_cls"] = cls_pred
                try:
                    probas = sgd.predict_proba(X2)
                    df["pred_confidence"] = np.max(probas, axis=1).round(3)
                except Exception:
                    df["pred_confidence"] = np.nan
                # top_terms JSON (ìƒìœ„ 5ê°œë¡œ ì¶•ì•½)
                rows = []
                for i in range(X2.shape[0]):
                    _, _, contribs = explain_top_terms(vec, sgd, X2[i], topk=5)
                    rows.append(json.dumps(contribs, ensure_ascii=False))
                df["top_terms"] = rows

            # í™”ë©´ í‘œì‹œ
            view_cols = []
            if "review_text" in df.columns: view_cols.append("review_text")
            if "review_date" in df.columns: view_cols.append("review_date")
            view_cols.append("pred_star_reg")
            if show_sgd and sgd is not None:
                view_cols += ["pred_star_cls","pred_confidence","top_terms"]

            df_view = df.loc[:, view_cols].rename(columns={
                "review_text":"ë¦¬ë·°","review_date":"ë‚ ì§œ","pred_star_reg":"RF ì˜ˆì¸¡ ë³„ì ",
                "pred_star_cls":"SGD ì˜ˆì¸¡ ë³„ì ","pred_confidence":"SGD í™•ì‹ ë„"
            })

            st.dataframe(df_view, use_container_width=True)

            # í‰ê·  & ìœ„í—˜ë„
            avg = float(df["pred_star_reg"].mean())
            level = risk_level(avg)
            c1, c2 = st.columns([1,1])
            with c1: st.metric("RF í‰ê·  í‰ì ", f"{avg:.2f} â˜…")
            with c2:
                st.markdown(
                    f"""<div style="padding:10px 12px;border-radius:10px;
                                   background:{risk_color(level)};color:#fff;
                                   display:inline-block;font-weight:600;">
                            ìœ„í—˜ë„: {level}
                        </div>""",
                    unsafe_allow_html=True,
                )

            # ë‹¤ìš´ë¡œë“œ
            st.download_button(
                "ê²°ê³¼ CSV ë‹¤ìš´ë¡œë“œ",
                df.to_csv(index=False, encoding="utf-8-sig"),
                file_name="predictions.csv",
                mime="text/csv",
            )
