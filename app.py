# app.py â€” Streamlit ë¦¬ë·° ì˜ˆì¸¡ ë°ëª¨ (skops ë¨¼ì €, ì—†ìœ¼ë©´ joblib)

import os, re
from pathlib import Path

import joblib
import numpy as np
import pandas as pd
import streamlit as st

# ---- skops ì˜µì…˜ ë¡œë“œ(ìˆìœ¼ë©´ ì‚¬ìš©) ----
try:
    from skops.io import load as skops_load, get_untrusted_types
    HAS_SKOPS = True
except Exception:
    HAS_SKOPS = False

ALLOWED_PREFIXES = ("sklearn.", "numpy.", "scipy.", "xgboost.", "lightgbm.")

def safe_skops_load(path: Path):
    """
    skops 0.10+ ë³´ì•ˆëª¨ë“œ: íŒŒì¼ ë‚´ íƒ€ì…ì„ ê²€ì‚¬í•˜ê³  trusted ëª©ë¡ìœ¼ë¡œ ë¡œë“œ
    """
    if not HAS_SKOPS:
        raise RuntimeError("skops ë¯¸ì„¤ì¹˜")
    p = str(path)
    # ì„œë¡œ ë‹¤ë¥¸ ì‹œê·¸ë‹ˆì²˜ ëŒ€ì‘
    try:
        types = get_untrusted_types(file=p)
    except TypeError:
        try:
            types = get_untrusted_types(path=p)
        except TypeError:
            types = get_untrusted_types()
    bad = [t for t in types if not t.startswith(ALLOWED_PREFIXES)]
    if bad:
        raise RuntimeError(f"í—ˆìš©ë˜ì§€ ì•Šì€ íƒ€ì… ê°ì§€: {bad[:5]}")
    return skops_load(p, trusted=types)

# =========================
#  ì „ì²˜ë¦¬ & í† í¬ë‚˜ì´ì¦ˆ
# =========================
POS_EMO = "ğŸ˜€ğŸ˜ƒğŸ˜„ğŸ˜ğŸ˜†ğŸ™‚ğŸ˜ŠğŸ˜ğŸ¤©ğŸ˜‹ğŸ˜‰ğŸ‘ğŸ™ŒğŸ‰â¤ğŸ’–ğŸ’—ğŸ’“ğŸ’ğŸ’•âœ¨ğŸ˜»ğŸ¥°ğŸ¤—ğŸ˜ºğŸ˜¸"
NEG_EMO = "ğŸ˜ğŸ˜ŸğŸ˜ ğŸ˜¡ğŸ˜¢ğŸ˜­ğŸ¤®ğŸ˜’ğŸ˜•ğŸ™â˜¹ğŸ‘ğŸ’¢ğŸ˜£ğŸ˜–ğŸ¤¬ğŸ˜¤ğŸ’”ğŸ˜¿ğŸ˜¹"
URL_RE = re.compile(r"(https?:\/\/[^\s]+)")
HTML_RE = re.compile(r"<[^>]+>")
MULTI_SPACE = re.compile(r"\s+")

def _replace_emojis(text: str) -> str:
    text = re.sub(f"[{re.escape(POS_EMO)}]+", " [EMO_POS] ", text)
    text = re.sub(f"[{re.escape(NEG_EMO)}]+", " [EMO_NEG] ", text)
    text = re.sub(r"[\U00010000-\U0010ffff]", " [EMO] ", text)
    return text

def clean_text(s: str) -> str:
    if not isinstance(s, str):
        return ""
    s = s.replace("\u200b", " ")
    s = HTML_RE.sub(" ", s)
    s = URL_RE.sub(" [URL] ", s)
    s = _replace_emojis(s)
    s = re.sub(r"[^0-9A-Za-zê°€-í£\.\,\!\?\[\]_ ]+", " ", s)
    s = MULTI_SPACE.sub(" ", s).strip()
    return s

def tokenize_and_join(s: str) -> str:
    # ë‘ ê¸€ì ì´ìƒ ì˜/ìˆ«/í•œê¸€ í† í°ë§Œ
    return " ".join(re.findall(r"[ê°€-í£A-Za-z0-9]{2,}", clean_text(s)))

# =========================
#  ëª¨ë¸ ê²½ë¡œ ì„ íƒ & ë¡œë“œ
# =========================
def _pick_models_dir() -> Path:
    here = Path(__file__).resolve().parent
    candidates = [
        here / "models",                # ê¶Œì¥
        Path("./models").resolve(),
        Path(os.getcwd()) / "models",
    ]
    for p in candidates:
        if p.exists():
            return p
    return candidates[0]

def _patch_rf_monotonic(reg):
    """sklearn 1.3â†’1.6 í˜¸í™˜ íŒ¨ì¹˜ (DecisionTreeRegressor.monotonic_cst)"""
    try:
        from sklearn.ensemble import RandomForestRegressor
        from sklearn.pipeline import Pipeline
        rf = None
        if isinstance(reg, Pipeline):
            last = reg.steps[-1][1]
            if isinstance(last, RandomForestRegressor):
                rf = last
        elif isinstance(reg, RandomForestRegressor):
            rf = reg
        if rf is not None and getattr(rf, "estimators_", None) is not None:
            for est in rf.estimators_:
                if not hasattr(est, "monotonic_cst"):
                    setattr(est, "monotonic_cst", None)
    except Exception:
        pass
    return reg

def _load_one(base: Path, stem: str):
    """
    ê°™ì€ ì´ë¦„ì˜ .skopsê°€ ìˆìœ¼ë©´ skopsë¡œ, ì•„ë‹ˆë©´ joblibë¡œ ë¡œë“œ
    stem: íŒŒì¼ ì•ë¶€ë¶„(ex. 'tfidf_vectorizer', 'sgd_logistic_cls', 'rf_reg')
    """
    sk = base / f"{stem}.skops"
    jl = base / f"{stem}.joblib"
    pkl = base / f"{stem}.pkl"

    if sk.exists() and HAS_SKOPS:
        return safe_skops_load(sk)
    if jl.exists():
        return joblib.load(jl)
    if pkl.exists():
        return joblib.load(pkl)
    raise FileNotFoundError(f"{stem}(.skops|.joblib|.pkl) íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

@st.cache_resource(show_spinner=True)
def load_models():
    BASE = _pick_models_dir()

    # ë°°í¬ ê²½ë¡œ ë””ë²„ê·¸(ë¬¸ì œì‹œ ë°”ë¡œ í™•ì¸)
    st.write("ğŸ” CWD:", os.getcwd())
    st.write("ğŸ“ BASE(models):", str(BASE))
    try:
        st.write("ğŸ“„ models contents:", sorted(p.name for p in BASE.glob("*")))
    except Exception:
        pass

    try:
        vectorizer = _load_one(BASE, "tfidf_vectorizer")
        cls        = _load_one(BASE, "sgd_logistic_cls")
        reg        = _load_one(BASE, "rf_reg")
    except FileNotFoundError as e:
        st.error(
            "ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\n"
            f"- ì°¾ì€ BASE í´ë”: {BASE}\n"
            f"- BASE ëª©ë¡: {[p.name for p in BASE.glob('*')]}\n"
            f"- ì—ëŸ¬: {e}\n\n"
            "ğŸ‘‰ ë¦¬í¬ì§€í† ë¦¬ ë£¨íŠ¸ì— `models/` í´ë”ë¥¼ ë‘ê³  ë‹¤ìŒ íŒŒì¼ë“¤ ì¤‘ í•˜ë‚˜ í˜•ì‹ìœ¼ë¡œ ì¤€ë¹„í•˜ì„¸ìš”.\n"
            "   â€¢ tfidf_vectorizer.(skops|pkl)\n"
            "   â€¢ sgd_logistic_cls.(skops|joblib)\n"
            "   â€¢ rf_reg.(skops|joblib)\n"
        )
        st.stop()

    reg = _patch_rf_monotonic(reg)
    return vectorizer, cls, reg

# =========================
#  Streamlit UI
# =========================
st.set_page_config(page_title="ë¦¬ë·° ì˜ˆì¸¡/ë¶„ì„", page_icon="â­", layout="wide")
st.title("â­ ë¦¬ë·° ë³„ì  ì˜ˆì¸¡ ë°ëª¨")

vec, cls, reg = load_models()

# â”€â”€ ë‹¨ì¼ ì˜ˆì¸¡
st.subheader("ë‹¨ì¼ í…ìŠ¤íŠ¸ ì˜ˆì¸¡")
inp = st.text_area("ë¦¬ë·° í…ìŠ¤íŠ¸ ì…ë ¥", height=160, placeholder="ë¦¬ë·°ë¥¼ ë¶™ì—¬ë„£ìœ¼ì„¸ìš”â€¦")
if st.button("ì˜ˆì¸¡í•˜ê¸°") and inp.strip():
    toks = tokenize_and_join(inp)
    X = vec.transform([toks])

    cls_star = int(cls.predict(X)[0])                   # ë¶„ë¥˜(ì •ìˆ˜)
    reg_star = float(np.clip(reg.predict(X)[0], 1, 5))  # íšŒê·€(ì—°ì†, 1~5 í´ë¦½)

    c1, c2 = st.columns(2)
    with c1: st.metric("ë¶„ë¥˜(ì •ìˆ˜)", f"{cls_star} â˜…")
    with c2: st.metric("íšŒê·€(ì—°ì†)", f"{reg_star:.2f} â˜…")

    # ê°„ë‹¨ ì„¤ëª…(ë¡œì§€ìŠ¤í‹±: ê°€ì¤‘ì¹˜ Ã— TF-IDF)
    if hasattr(cls, "coef_"):
        feats = vec.get_feature_names_out()
        try:
            proba = cls.predict_proba(X)[0]
            st.caption(f"ìµœëŒ€ í´ë˜ìŠ¤ í™•ë¥ : {np.max(proba):.3f}")
        except Exception:
            pass
        y_label = cls.predict(X)[0]
        classes = getattr(cls, "classes_", None)
        y_idx = int(np.where(classes == y_label)[0][0]) if classes is not None else int(y_label) - 1
        coef = cls.coef_[y_idx]
        xv = X.toarray().ravel()
        contrib = coef * xv
        idx = np.argsort(contrib)[::-1]
        rows = []
        for j in idx:
            if xv[j] == 0:
                continue
            rows.append({
                "term": str(feats[j]),
                "tfidf": float(xv[j]),
                "coef": float(coef[j]),
                "contrib": float(contrib[j]),
            })
            if len(rows) >= 8:
                break
        st.dataframe(pd.DataFrame(rows), use_container_width=True)

st.divider()

# â”€â”€ ë°°ì¹˜ ì˜ˆì¸¡
st.subheader("ë°°ì¹˜ ì˜ˆì¸¡ (CSV ì—…ë¡œë“œ)")
csv = st.file_uploader("CSV ì—…ë¡œë“œ (í•„ìˆ˜ ì»¬ëŸ¼: review_text)", type=["csv"])
if csv is not None:
    try:
        df = pd.read_csv(csv)
    except Exception as e:
        st.error(f"CSV ë¡œë”© ì‹¤íŒ¨: {e}")
    else:
        if "review_text" not in df.columns:
            st.error("CSVì— 'review_text' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        else:
            toks = df["review_text"].fillna("").astype(str).map(tokenize_and_join)
            X = vec.transform(toks)
            df["pred_star_cls"] = cls.predict(X)
            df["pred_star_reg"] = np.clip(reg.predict(X), 1, 5).round(2)

            st.dataframe(df.head(50), use_container_width=True)
            st.download_button(
                "ê²°ê³¼ CSV ë‹¤ìš´ë¡œë“œ",
                df.to_csv(index=False, encoding="utf-8-sig"),
                file_name="predictions.csv",
                mime="text/csv",
            )
