# app.py
# -*- coding: utf-8 -*-

from __future__ import annotations
import re
from pathlib import Path

import joblib
import numpy as np
import pandas as pd
import streamlit as st

# ---- (ì„ íƒ) skops: ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš© ----
try:
    from skops.io import load as skops_load, get_untrusted_types
    HAS_SKOPS = True
except Exception:
    HAS_SKOPS = False

ALLOWED_PREFIXES = ("sklearn.", "numpy.", "scipy.", "xgboost.", "lightgbm.")

def safe_skops_load(path: Path):
    if not HAS_SKOPS:
        raise RuntimeError("skops ë¯¸ì„¤ì¹˜")
    p = str(path)
    try:
        types = get_untrusted_types(file=p)
    except TypeError:
        try:
            types = get_untrusted_types(path=p)
        except TypeError:
            types = get_untrusted_types()
    # ë³´ì•ˆ: ìš°ë¦¬ê°€ ì“¸ ê°€ëŠ¥ì„±ì´ ìˆëŠ” ëª¨ë“ˆë§Œ í—ˆìš©
    _ = [t for t in types if t.startswith(ALLOWED_PREFIXES)]
    return skops_load(p, trusted=types)

# -------------------- ê¸°ë³¸ ì„¤ì • --------------------
st.set_page_config(page_title="ë¦¬ë·° ì˜ˆì¸¡/ë¶„ì„", page_icon="â­", layout="wide")

# -------------------- ëª¨ë¸ ê²½ë¡œ --------------------
ROOT = Path(__file__).resolve().parent
MODELS = ROOT / "models"

VEC_SKOPS = MODELS / "tfidf_vectorizer.skops"
VEC_PKL   = MODELS / "tfidf_vectorizer.pkl"
REG_SKOPS = MODELS / "rf_reg.skops"
REG_JBL   = MODELS / "rf_reg.joblib"

def _assert_files_exist():
    have_vec = VEC_SKOPS.exists() or VEC_PKL.exists()
    have_reg = REG_SKOPS.exists() or REG_JBL.exists()
    if not (have_vec and have_reg):
        st.error(
            "ëª¨ë¸ íŒŒì¼ ëˆ„ë½ì…ë‹ˆë‹¤. models/ í´ë”ì— ë‹¤ìŒ ì¤‘ í•˜ë‚˜ì”©ì€ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.\n\n"
            "â€¢ tfidf_vectorizer.(skops | pkl)\n"
            "â€¢ rf_reg.(skops | joblib)\n"
        )
        st.stop()

# -------------------- ì „ì²˜ë¦¬/í† í¬ë‚˜ì´ì¦ˆ --------------------
POS_EMO = "ğŸ˜€ğŸ˜ƒğŸ˜„ğŸ˜ğŸ˜†ğŸ™‚ğŸ˜ŠğŸ˜ğŸ¤©ğŸ˜‹ğŸ˜‰ğŸ‘ğŸ™ŒğŸ‰â¤ğŸ’–ğŸ’—ğŸ’“ğŸ’ğŸ’•âœ¨ğŸ˜»ğŸ¥°ğŸ¤—ğŸ˜ºğŸ˜¸"
NEG_EMO = "ğŸ˜ğŸ˜ŸğŸ˜ ğŸ˜¡ğŸ˜¢ğŸ˜­ğŸ¤®ğŸ˜’ğŸ˜•ğŸ™â˜¹ğŸ‘ğŸ’¢ğŸ˜£ğŸ˜–ğŸ¤¬ğŸ˜¤ğŸ’”ğŸ˜¿ğŸ˜¹"
URL_RE = re.compile(r"(https?:\/\/[^\s]+)")
HTML_RE = re.compile(r"<[^>]+>")
MULTI_SPACE = re.compile(r"\s+")

def _replace_emojis(text: str) -> str:
    text = re.sub(f"[{re.escape(POS_EMO)}]+", " [EMO_POS] ", text)
    text = re.sub(f"[{re.escape(NEG_EMO)}]+", " [EMO_NEG] ", text)
    text = re.sub(r"[\U00010000-\U0010ffff]", " [EMO] ", text)
    return text

def _clean_text(s: str) -> str:
    if not isinstance(s, str):
        return ""
    s = s.replace("\u200b", " ")
    s = HTML_RE.sub(" ", s)
    s = URL_RE.sub(" [URL] ", s)
    s = _replace_emojis(s)
    s = re.sub(r"[^0-9A-Za-zê°€-í£\.\,\!\?\[\]_ ]+", " ", s)
    s = MULTI_SPACE.sub(" ", s).strip()
    return s

def tokenize_and_join(s: str) -> str:
    return " ".join(re.findall(r"[ê°€-í£A-Za-z0-9]{2,}", _clean_text(s)))

# -------------------- í˜¸í™˜ íŒ¨ì¹˜: RF monotonic_cst --------------------
def _patch_rf_monotonic(reg_model):
    try:
        from sklearn.ensemble import RandomForestRegressor
        from sklearn.pipeline import Pipeline
        rf = None
        if isinstance(reg_model, Pipeline):
            last = reg_model.steps[-1][1]
            if isinstance(last, RandomForestRegressor):
                rf = last
        elif isinstance(reg_model, RandomForestRegressor):
            rf = reg_model
        if rf is not None and getattr(rf, "estimators_", None) is not None:
            for est in rf.estimators_:
                if not hasattr(est, "monotonic_cst"):
                    setattr(est, "monotonic_cst", None)
    except Exception:
        pass
    return reg_model

# -------------------- ëª¨ë¸ ë¡œë” (ìºì‹œ) --------------------
@st.cache_resource(show_spinner=True)
def load_models():
    _assert_files_exist()

    # ë²¡í„°ë¼ì´ì €
    if VEC_SKOPS.exists() and HAS_SKOPS:
        vectorizer = safe_skops_load(VEC_SKOPS)
    else:
        vectorizer = joblib.load(VEC_PKL)

    # íšŒê·€ ëª¨ë¸
    if REG_SKOPS.exists() and HAS_SKOPS:
        reg = safe_skops_load(REG_SKOPS)
    else:
        reg = joblib.load(REG_JBL)

    reg = _patch_rf_monotonic(reg)
    return vectorizer, reg

# ==========================================================
#                           UI
# ==========================================================
st.title("â­ ë¦¬ë·° ì˜ˆì¸¡ ë°ëª¨")
vec, reg = load_models()

# ---------------- ë‹¨ì¼ ì˜ˆì¸¡ ----------------
st.subheader("ë‹¨ì¼ í…ìŠ¤íŠ¸ ì˜ˆì¸¡")
inp = st.text_area("ë¦¬ë·° í…ìŠ¤íŠ¸ ì…ë ¥", height=160, placeholder="ë¦¬ë·°ë¥¼ ë¶™ì—¬ë„£ìœ¼ì„¸ìš”â€¦")

if st.button("ì˜ˆì¸¡í•˜ê¸°") and inp.strip():
    toks = tokenize_and_join(inp)
    X = vec.transform([toks])

    pred_score = float(np.clip(reg.predict(X)[0], 1, 5))  # 1~5 ë²”ìœ„ë¡œ í´ë¦½
    st.metric("ì˜ˆì¸¡ ì ìˆ˜", f"{pred_score:.2f} â˜…")

st.divider()

# ---------------- ë°°ì¹˜ ì˜ˆì¸¡ ----------------
st.subheader("ë°°ì¹˜ ì˜ˆì¸¡ (CSV ì—…ë¡œë“œ)")
csv = st.file_uploader("CSV ì—…ë¡œë“œ (í•„ìˆ˜ ì»¬ëŸ¼: review_text)", type=["csv"])

if csv is not None:
    try:
        df = pd.read_csv(csv)
    except Exception as e:
        st.error(f"CSV ë¡œë”© ì‹¤íŒ¨: {e}")
    else:
        if "review_text" not in df.columns:
            st.error("CSVì— 'review_text' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        else:
            toks = df["review_text"].fillna("").astype(str).map(tokenize_and_join)
            X = vec.transform(toks)
            df["pred_score"] = np.clip(reg.predict(X), 1, 5).round(2)

            st.dataframe(df.head(50), use_container_width=True)
            st.download_button(
                "ê²°ê³¼ CSV ë‹¤ìš´ë¡œë“œ",
                df.to_csv(index=False, encoding="utf-8-sig"),
                file_name="predictions.csv",
                mime="text/csv",
            )
