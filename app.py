# app.py â€” Streamlit ë¦¬ë·° ì˜ˆì¸¡(íšŒê·€ + Kiwi + neg/pos + SGDê¸°ë°˜ ë¶€ì • í† í° TOP3)
# -*- coding: utf-8 -*-

from __future__ import annotations
import json, re
from pathlib import Path

import joblib
import numpy as np
import pandas as pd
import streamlit as st

# =========================
# ê¸°ë³¸ ì„¤ì •
# =========================
st.set_page_config(page_title="ë¦¬ë·° ì˜ˆì¸¡/ë¶„ì„", page_icon="â­", layout="wide")

ROOT = Path(__file__).resolve().parent
MODELS = ROOT / "models"

# ë²¡í„°/ëª¨ë¸ ê²½ë¡œ (pkl/skops ëª¨ë‘ ì§€ì›)
VEC_PKL   = MODELS / "tfidf_vectorizer.pkl"
VEC_SKOPS = MODELS / "tfidf_vectorizer.skops"
REG_JOB   = MODELS / "rf_reg.joblib"
REG_SKOPS = MODELS / "rf_reg.skops"
SGD_JOB   = MODELS / "sgd_logistic_cls.joblib"
SGD_SKOPS = MODELS / "sgd_logistic_cls.skops"
SUMMARY   = MODELS / "tfidf_summary.json"

# (ì„ íƒ) skops ë¡œë”
ALLOWED_PREFIXES = ("sklearn.", "numpy.", "scipy.", "xgboost.", "lightgbm.")
try:
    from skops.io import load as skops_load, get_untrusted_types
    HAS_SKOPS = True
except Exception:
    HAS_SKOPS = False


def _assert_files_exist(paths):
    miss = [p for p in paths if not p.exists()]
    if miss:
        st.error(f"ëª¨ë¸ íŒŒì¼ ëˆ„ë½: {[str(p) for p in miss]}")
        st.stop()


def _safe_skops_load(path: Path):
    p = str(path)
    try:
        types = get_untrusted_types(file=p)
    except TypeError:
        try:
            types = get_untrusted_types(path=p)
        except TypeError:
            types = get_untrusted_types()
    bad = [t for t in types if not t.startswith(ALLOWED_PREFIXES)]
    if bad:
        raise RuntimeError(
            "skops íŒŒì¼ì— ë¹„í—ˆìš© íƒ€ì…ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n"
            f"- íŒŒì¼: {p}\n- ë¹„í—ˆìš© ì˜ˆ: {bad[:5]} ..."
        )
    return skops_load(p, trusted=types)


def _patch_rf_monotonic(reg_model):
    try:
        from sklearn.ensemble import RandomForestRegressor
        from sklearn.pipeline import Pipeline
        rf = None
        if isinstance(reg_model, Pipeline):
            last = reg_model.steps[-1][1]
            if isinstance(last, RandomForestRegressor):
                rf = last
        elif isinstance(reg_model, RandomForestRegressor):
            rf = reg_model
        if rf is not None and getattr(rf, "estimators_", None) is not None:
            for est in rf.estimators_:
                if not hasattr(est, "monotonic_cst"):
                    setattr(est, "monotonic_cst", None)
    except Exception:
        pass
    return reg_model


# =========================
# ì „ì²˜ë¦¬/í† í¬ë‚˜ì´ì¦ˆ (Kiwi ê³ ì •)
# =========================
POS_EMO = "ğŸ˜€ğŸ˜ƒğŸ˜„ğŸ˜ğŸ˜†ğŸ™‚ğŸ˜ŠğŸ˜ğŸ¤©ğŸ˜‹ğŸ˜‰ğŸ‘ğŸ™ŒğŸ‰â¤ğŸ’–ğŸ’—ğŸ’“ğŸ’ğŸ’•âœ¨ğŸ˜»ğŸ¥°ğŸ¤—ğŸ˜ºğŸ˜¸"
NEG_EMO = "ğŸ˜ğŸ˜ŸğŸ˜ ğŸ˜¡ğŸ˜¢ğŸ˜­ğŸ¤®ğŸ˜’ğŸ˜•ğŸ™â˜¹ğŸ‘ğŸ’¢ğŸ˜£ğŸ˜–ğŸ¤¬ğŸ˜¤ğŸ’”ğŸ˜¿ğŸ˜¹"
URL_RE = re.compile(r"(https?:\/\/[^\s]+)")
HTML_RE = re.compile(r"<[^>]+>")
MULTI_SPACE = re.compile(r"\s+")


def _replace_emojis(text: str) -> str:
    text = re.sub(f"[{re.escape(POS_EMO)}]+", " [EMO_POS] ", text)
    text = re.sub(f"[{re.escape(NEG_EMO)}]+", " [EMO_NEG] ", text)
    text = re.sub(r"[\U00010000-\U0010ffff]", " [EMO] ", text)
    return text


def _clean_text(s: str) -> str:
    if not isinstance(s, str):
        return ""
    s = s.replace("\u200b", " ")
    s = HTML_RE.sub(" ", s)
    s = URL_RE.sub(" [URL] ", s)
    s = _replace_emojis(s)
    s = re.sub(r"[^0-9A-Za-zê°€-í£\.\,\!\?\[\]_ ]+", " ", s)
    s = MULTI_SPACE.sub(" ", s).strip()
    return s


# ---- Kiwi í† í¬ë‚˜ì´ì € ê³ ì • (predict_safe.pyì™€ ë™ì¼ í’ˆì‚¬êµ°) ----
try:
    from kiwipiepy import Kiwi
    _KIWI = Kiwi()
    _TOKENIZER_NAME = "kiwi"
except Exception as e:
    raise RuntimeError(
        "Kiwi í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. "
        "requirements.txtì— 'kiwipiepy'ë¥¼ ì¶”ê°€/ì„¤ì¹˜í•˜ê³  ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”."
    ) from e


def _tokenize(text: str) -> list[str]:
    return [t.form for t in _KIWI.tokenize(text)
            if t.tag.startswith(("N", "V", "MAG", "IC", "XR", "MM"))]


def tokenize_and_join(s: str, stopwords: set[str] | None = None) -> str:
    toks = _tokenize(_clean_text(s))
    if stopwords:
        toks = [t for t in toks if t not in stopwords]
    return " ".join(toks)


# =========================
# neg/pos ì—´ê°€ì¤‘
# =========================
def _apply_column_scaling_csc(X_csc, vocab: dict, terms: list, scale: float):
    hits = [vocab[t] for t in terms if t in vocab]
    for j in hits:
        start, end = X_csc.indptr[j], X_csc.indptr[j + 1]
        if end > start:
            X_csc.data[start:end] *= scale
    return X_csc


def maybe_apply_negpos_bonus(X_csr, vec, base_dir: Path):
    summ = base_dir / "tfidf_summary.json"
    if not summ.exists():
        return X_csr
    try:
        with open(summ, "r", encoding="utf-8") as f:
            js = json.load(f)
        info = js.get("tfidf", {})
        neg_terms = info.get("neg_terms", []) or []
        pos_terms = info.get("pos_terms", []) or []
        neg_bonus = float(info.get("neg_bonus", 1.0))
        pos_bonus = float(info.get("pos_bonus", 1.0))
        if (neg_terms or pos_terms) and hasattr(vec, "vocabulary_"):
            vocab = vec.vocabulary_
            X_csc = X_csr.tocsc(copy=True)
            if neg_terms and neg_bonus != 1.0:
                X_csc = _apply_column_scaling_csc(X_csc, vocab, neg_terms, neg_bonus)
            if pos_terms and pos_bonus != 1.0:
                X_csc = _apply_column_scaling_csc(X_csc, vocab, pos_terms, pos_bonus)
            return X_csc.tocsr(copy=False)
    except Exception:
        return X_csr
    return X_csr


# =========================
# ëª¨ë¸ ë¡œë“œ (ìºì‹œ)
# =========================
@st.cache_resource(show_spinner=True)
def load_models():
    # ë²¡í„°ë¼ì´ì €
    if VEC_SKOPS.exists() and HAS_SKOPS:
        vec = _safe_skops_load(VEC_SKOPS)
    else:
        _assert_files_exist([VEC_PKL])
        vec = joblib.load(VEC_PKL)

    # RF íšŒê·€
    if REG_SKOPS.exists() and HAS_SKOPS:
        reg = _safe_skops_load(REG_SKOPS)
    else:
        _assert_files_exist([REG_JOB])
        reg = joblib.load(REG_JOB)
    reg = _patch_rf_monotonic(reg)

    # SGD ë¶„ë¥˜ (ì„¤ëª…ìš©)
    cls = None
    if SGD_SKOPS.exists() and HAS_SKOPS:
        cls = _safe_skops_load(SGD_SKOPS)
    elif SGD_JOB.exists():
        cls = joblib.load(SGD_JOB)

    # stopwords
    stop = set()
    for sw in [ROOT.parent / "stopwords_ko.txt",
               ROOT / "stopwords_ko.txt",
               MODELS / "stopwords_ko.txt",
               MODELS / "stopwords_ko1.txt"]:
        if Path(sw).exists():
            with open(sw, encoding="utf-8") as f:
                stop = {x.strip() for x in f if x.strip()}
            break

    dbg = {
        "tokenizer": _TOKENIZER_NAME,
        "stopwords": len(stop),
        "vectorizer_features": len(vec.get_feature_names_out()) if hasattr(vec, "get_feature_names_out") else None,
        "rf_n_features_in_": getattr(reg, "n_features_in_", None),
        "negpos_summary": SUMMARY.exists(),
        "has_sgd": cls is not None,
    }
    return vec, reg, cls, stop, dbg


# =========================
# ìœ„í—˜ë„ íŒì •
# =========================
def risk_level(avg_score: float) -> str:
    if avg_score > 4.06:
        return "Safe"
    if avg_score > 3.84:
        return "Low"
    if avg_score > 3.60:
        return "Medium"
    return "High"


def risk_color(level: str) -> str:
    return {
        "Safe": "#2e7d32",
        "Low": "#558b2f",
        "Medium": "#f9a825",
        "High": "#c62828",
    }.get(level, "#333333")


# =========================
# ë¶€ì • í† í° TOP3 (ë„¤ê°€ ë§í•œ ë°©ì‹ + í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸)
# =========================
def worst_tokens_from_batch_by_margin(X, vec, cls, stopwords: set[str], topk=3):
    """
    X: (n_samples, n_features) sparse
    vec: vectorizer
    cls: SGDClassifier
    stopwords: ìš°ë¦¬ê°€ ì½ì–´ì˜¨ ë¶ˆìš©ì–´ ì„¸íŠ¸
    """
    if cls is None or not hasattr(cls, "coef_"):
        return []

    feats = vec.get_feature_names_out()
    coef = cls.coef_
    classes = getattr(cls, "classes_", np.arange(coef.shape[0]))

    # 1ì  / 5ì  ê³„ìˆ˜ ì°¾ê¸°
    i1 = np.where(classes == 1)[0]
    i5 = np.where(classes == 5)[0]
    if not len(i1) or not len(i5):
        return []

    w1 = coef[int(i1[0])]
    w5 = coef[int(i5[0])]
    delta = w5 - w1  # (coef5 - coef1)

    # â”€â”€ ì—¬ê¸° ì¶”ê°€ëœ í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ â”€â”€
    DISPLAY_WHITELIST = {
        # ì„œë¹„ìŠ¤/ì‘ëŒ€
        "ì„œë¹„ìŠ¤", "ë¶ˆì¹œì ˆ", "ì¹œì ˆ", "ì‘ëŒ€", "ì§ì›", "ì‚¬ì¥", "ì‚¬ì¥ë‹˜",
        # ê°€ê²©/ê°€ì„±ë¹„
        "ê°€ê²©", "ë¹„ì‹¸", "ì €ë ´", "ê°€ì„±ë¹„", "í• ì¸",
        # ìœ„ìƒ/ì²­ê²°/ì´ë¬¼ì§ˆ
        "ìœ„ìƒ", "ì²­ê²°", "ë”ëŸ½", "ë”ëŸ¬", "ê¹¨ë—", "ê¹”ë”", "ëƒ„ìƒˆ", "ë²Œë ˆ", "ë¨¸ë¦¬ì¹´ë½", "ì´ë¬¼ì§ˆ",
        # ëŒ€ê¸°/ì ‘ê·¼ì„±
        "ëŒ€ê¸°", "ì›¨ì´íŒ…", "ì¤„", "ì£¼ì°¨", "ìë¦¬",
        # ë§›/í’ˆì§ˆ
        "ë§›", "ë§›ì—†", "ì‹ ì„ ", "í€„ë¦¬í‹°", "ì‹ê°",
        # ì–‘/êµ¬ì„±
        "ì–‘", "êµ¬ì„±", "ì°½ë ¬",
        # ë¶„ìœ„ê¸°
        "ë¶„ìœ„ê¸°", "ì‹œë„ëŸ½","ê³µê°„", "ì¸í…Œë¦¬ì–´",
        # ê¸°íƒ€ ë¶€ì •
        "ì˜¤ë˜", "ëŠë¦¼"
    }
    
    BLOCKLIST = {
        "ë‹¤ì‹œ","ì§„ì§œ","ì •ë§","ì•„ì‰½",
    }

    agg = {}
    for i in range(X.shape[0]):
        row = X[i].toarray().ravel()
        contrib = delta * row
        neg_idx = np.where(contrib < 0)[0]
        for j in neg_idx:
            if row[j] == 0:
                continue
            agg[j] = agg.get(j, 0.0) + contrib[j]

    if not agg:
        return []

    sorted_items = sorted(agg.items(), key=lambda kv: kv[1])  # ê°€ì¥ ìŒìˆ˜ë¶€í„°

    cleaned: list[tuple[str, float]] = []
    for j, val in sorted_items:
        tok = feats[j]

        # ìš°ì„  ë¶ˆìš©ì–´ / ë¸”ë™ë¦¬ìŠ¤íŠ¸ / í•œê¸€ì / ìˆ«ìÂ·ê¸°í˜¸ ì œê±°
        if tok in stopwords or tok in BLOCKLIST:
            continue
        if len(tok) < 2:
            continue
        if re.fullmatch(r"[0-9\W_]+", tok):
            continue

        # í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ì•ˆì— ìˆìœ¼ë©´ ë°”ë¡œ í›„ë³´
        if tok in DISPLAY_WHITELIST:
            cleaned.append((tok, val))

        if len(cleaned) >= topk:
            break

    # í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ë¡œ 3ê°œë¥¼ ëª» ì±„ì› ìœ¼ë©´ ë‚¨ì€ ê²ƒë“¤ì—ì„œ ê·¸ëƒ¥ ì±„ì›€
    if len(cleaned) < topk:
        for j, val in sorted_items:
            tok = feats[j]
            # ì—¬ê¸°ì„œë„ ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì œê±°
            if tok in stopwords or tok in BLOCKLIST or len(tok) < 2:
                continue
            if re.fullmatch(r"[0-9\W_]+", tok):
                continue
            if (tok, val) in cleaned:
                continue
            cleaned.append((tok, val))
            if len(cleaned) >= topk:
                break

    labels = [f"{tok}" for tok, val in cleaned]
    return labels


# ==========================================================
#                          UI
# ==========================================================
st.title("â­ ê°€ê²Œ ë¦¬ë·° ì˜ˆì¸¡ â­")

vec, reg, cls, stopwords, dbg = load_models()

# â”€â”€ ë‹¨ì¼ ì˜ˆì¸¡
st.subheader("ë‹¨ì¼ í…ìŠ¤íŠ¸ ì˜ˆì¸¡")
inp = st.text_area("ë¦¬ë·° í…ìŠ¤íŠ¸ ì…ë ¥", height=160, placeholder="ë¦¬ë·°ë¥¼ ë¶™ì—¬ë„£ìœ¼ì„¸ìš”â€¦")
if st.button("ì˜ˆì¸¡í•˜ê¸°") and inp.strip():
    toks = tokenize_and_join(inp, stopwords)
    X = vec.transform([toks])
    X = maybe_apply_negpos_bonus(X, vec, MODELS)
    score = float(np.clip(reg.predict(X)[0], 1, 5))
    st.metric("ì˜ˆì¸¡ ì ìˆ˜", f"{score:.2f} â˜…")

st.divider()

# â”€â”€ ë°°ì¹˜ ì˜ˆì¸¡
st.subheader("ëŒ€ëŸ‰ ë¦¬ë·° ì˜ˆì¸¡ (CSV ì—…ë¡œë“œ)")
csv = st.file_uploader("CSV ì—…ë¡œë“œ (í•„ìˆ˜ Column: review_text)", type=["csv"])

if csv is not None:
    try:
        df = pd.read_csv(csv)
    except Exception as e:
        st.error(f"CSV ë¡œë”© ì‹¤íŒ¨: {e}")
    else:
        if "review_text" not in df.columns:
            st.error("CSVì— 'review_text' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        else:
            toks = df["review_text"].fillna("").astype(str).map(lambda s: tokenize_and_join(s, stopwords))
            X = vec.transform(toks)
            Xb = maybe_apply_negpos_bonus(X, vec, MODELS)
            df["pred_score"] = np.clip(reg.predict(Xb), 1, 5).round(2)

            # í‘œì‹œìš©
            view_cols = []
            if "review_text" in df.columns:
                view_cols.append("review_text")
            if "review_date" in df.columns:
                view_cols.append("review_date")
            view_cols.append("pred_score")

            df_view = df.loc[:, view_cols].rename(
                columns={
                    "review_text": "ë¦¬ë·°",
                    "review_date": "ë‚ ì§œ",
                    "pred_score": "ì˜ˆì¸¡ ë³„ì ",
                }
            )
            st.dataframe(df_view, use_container_width=True)

            # í‰ê·  & ìœ„í—˜ë„
            avg = float(df["pred_score"].mean())
            level = risk_level(avg)
            col1, col2 = st.columns([1, 1])
            with col1:
                st.metric("í‰ê·  í‰ì ", f"{avg:.2f} â˜…")
            with col2:
                st.markdown(
                    f"""
                    <div style="padding:10px 12px;border-radius:10px;
                                background:{risk_color(level)};color:#fff;
                                display:inline-block;font-weight:600;">
                        ìœ„í—˜ë„: {level}
                    </div>
                    """,
                    unsafe_allow_html=True,
                )

            # ë¶€ì • í† í° TOP3
            if level in {"High", "Medium"}:
                if cls is not None:
                    worst3 = worst_tokens_from_batch_by_margin(Xb, vec, cls, stopwords, topk=3)
                    st.markdown("**ë¦¬ë·° ì† ë¶€ì •ì  ë‹¨ì–´ TOP3:** " + ", ".join(worst3))
                else:
                    st.markdown("**ë¦¬ë·° ì† ë¶€ì •ì  ë‹¨ì–´ TOP3:** ë¶„ë¥˜ ëª¨ë¸ì´ ì—†ì–´ í‘œì‹œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

            # ë‹¤ìš´ë¡œë“œ
            st.download_button(
                "ê²°ê³¼ CSV ë‹¤ìš´ë¡œë“œ",
                df.to_csv(index=False, encoding="utf-8-sig"),
                file_name="predictions.csv",
                mime="text/csv",
            )
