# app.py â€” Streamlit ë¦¬ë·° ì˜ˆì¸¡(ìŠ¤í¬ë¦½íŠ¸ì™€ ì •í•©, RFì˜ˆì¸¡ + SGDì„¤ëª…)
# -*- coding: utf-8 -*-
from __future__ import annotations
import re, json
from pathlib import Path
from collections import defaultdict

import numpy as np
import pandas as pd
import streamlit as st
import joblib
from scipy.sparse import csr_matrix

# ============= ê¸°ë³¸ UI =============
st.set_page_config(page_title="ë¦¬ë·° ì˜ˆì¸¡/ë¶„ì„", page_icon="â­", layout="wide")
st.title("â­ ë¦¬ë·° ì˜ˆì¸¡")

ROOT   = Path(__file__).resolve().parent
MODELS = ROOT / "models"

ALLOWED_PREFIXES = ("sklearn.", "numpy.", "scipy.", "xgboost.", "lightgbm.")

# ============= skops ì•ˆì „ ë¡œë” =============
try:
    from skops.io import load as skops_load, get_untrusted_types
    HAS_SKOPS = True
except Exception:
    HAS_SKOPS = False

def safe_skops_load(path: Path):
    p = str(path)
    types = None
    try:
        types = get_untrusted_types(file=p)
    except TypeError:
        try:
            types = get_untrusted_types(path=p)
        except TypeError:
            types = get_untrusted_types()
    bad = [t for t in types if not t.startswith(ALLOWED_PREFIXES)]
    if bad:
        raise RuntimeError(f"ë¹„í—ˆìš© íƒ€ì… í¬í•¨: {bad[:5]} in {p}")
    return skops_load(p, trusted=types)

def _patch_rf_monotonic(reg_pipeline):
    try:
        from sklearn.ensemble import RandomForestRegressor
        from sklearn.pipeline import Pipeline
        rf = None
        if isinstance(reg_pipeline, Pipeline):
            last = reg_pipeline.steps[-1][1]
            if isinstance(last, RandomForestRegressor):
                rf = last
        elif isinstance(reg_pipeline, RandomForestRegressor):
            rf = reg_pipeline
        if rf is None or getattr(rf, "estimators_", None) is None:
            return reg_pipeline
        for est in rf.estimators_:
            if not hasattr(est, "monotonic_cst"):
                setattr(est, "monotonic_cst", None)
    except Exception:
        pass
    return reg_pipeline

def load_vectorizer_with_compat(base: Path, model_expected_n: int | None):
    cands = []
    sk = base / "tfidf_vectorizer.skops"
    pk = base / "tfidf_vectorizer.pkl"
    if sk.exists() and HAS_SKOPS:
        cands.append(("skops", sk))
    if pk.exists():
        cands.append(("pkl", pk))
    if not cands:
        raise FileNotFoundError("tfidf_vectorizer.(skops|pkl) ì—†ìŒ")

    last_err, chosen = None, None
    for kind, path in cands:
        try:
            vec = safe_skops_load(path) if kind == "skops" else joblib.load(path)
            n_vec = len(vec.get_feature_names_out())
            if (model_expected_n is None) or (n_vec == model_expected_n):
                chosen = vec
                break
            else:
                last_err = f"{path.name}: {n_vec} vs expected {model_expected_n}"
        except Exception as e:
            last_err = f"load fail {path.name}: {e!r}"
    if chosen is None:
        raise ValueError(f"í˜¸í™˜ ë²¡í„°ë¼ì´ì € ì—†ìŒ. ë§ˆì§€ë§‰ ì˜¤ë¥˜: {last_err}")
    return chosen

# ============= í† í¬ë‚˜ì´ì¦ˆ/ì •ê·œí™” (ìŠ¤í¬ë¦½íŠ¸ ë™ì¼) =============
POS_EMO = "ğŸ˜€ğŸ˜ƒğŸ˜„ğŸ˜ğŸ˜†ğŸ™‚ğŸ˜ŠğŸ˜ğŸ¤©ğŸ˜‹ğŸ˜‰ğŸ‘ğŸ™ŒğŸ‰â¤ğŸ’–ğŸ’—ğŸ’“ğŸ’ğŸ’•âœ¨ğŸ˜»ğŸ¥°ğŸ¤—ğŸ˜ºğŸ˜¸"
NEG_EMO = "ğŸ˜ğŸ˜ŸğŸ˜ ğŸ˜¡ğŸ˜¢ğŸ˜­ğŸ¤®ğŸ˜’ğŸ˜•ğŸ™â˜¹ğŸ‘ğŸ’¢ğŸ˜£ğŸ˜–ğŸ¤¬ğŸ˜¤ğŸ’”ğŸ˜¿ğŸ˜¹"
URL_RE   = re.compile(r"(https?:\/\/[^\s]+)")
HTML_RE  = re.compile(r"<[^>]+>")
MULTI_S  = re.compile(r"\s+")

def replace_emojis(text: str) -> str:
    text = re.sub(f"[{re.escape(POS_EMO)}]+", " [EMO_POS] ", text)
    text = re.sub(f"[{re.escape(NEG_EMO)}]+", " [EMO_NEG] ", text)
    text = re.sub(r"[\U00010000-\U0010ffff]", " [EMO] ", text)
    return text

def clean_text(s: str) -> str:
    if not isinstance(s, str):
        return ""
    s = s.replace("\u200b", " ")
    s = HTML_RE.sub(" ", s)
    s = URL_RE.sub(" [URL] ", s)
    s = replace_emojis(s)
    s = re.sub(r"[^0-9A-Za-zê°€-í£\.\,\!\?\[\]_ ]+", " ", s)
    s = MULTI_S.sub(" ", s).strip()
    return s

def get_tokenizer():
    # 1) mecab
    try:
        from mecab import MeCab
        m = MeCab()
        def tok(text):
            return [w for (w, p) in m.pos(text) if p.startswith(("NN","VV","VA","MAG","IC","XR"))]
        return "mecab_python", tok
    except Exception:
        pass
    # 2) kiwi
    try:
        from kiwipiepy import Kiwi
        kiwi = Kiwi()
        def tok(text):
            return [t.form for t in kiwi.tokenize(text) if t.tag.startswith(("N","V","MAG","IC","XR","MM"))]
        return "kiwi", tok
    except Exception:
        pass
    # 3) fallback
    def tok(text):
        return re.findall(r"[ê°€-í£A-Za-z0-9]{2,}", text)
    return "simple", tok

TOK_NAME, TOKENIZE = get_tokenizer()

def tokenize_and_join(text: str, stopwords:set) -> str:
    toks = TOKENIZE(clean_text(text))
    if stopwords:
        toks = [t for t in toks if t not in stopwords]
    return " ".join(toks)

# ============= neg/pos ë³´ë„ˆìŠ¤ (ìŠ¤í¬ë¦½íŠ¸ ë™ì¼) =============
def apply_column_scaling_csc(X_csc, vocab: dict, terms: list, scale: float):
    hits = [vocab[t] for t in terms if t in vocab]
    for j in hits:
        start, end = X_csc.indptr[j], X_csc.indptr[j+1]
        if end > start:
            X_csc.data[start:end] *= scale
    return X_csc

def maybe_apply_negpos_bonus(X_csr, vec, base_dir: Path):
    summ = base_dir / "tfidf_summary.json"
    if not summ.exists():
        return X_csr
    try:
        js = json.loads(summ.read_text(encoding="utf-8"))
        info = js.get("tfidf", {})
        neg_terms = info.get("neg_terms", []) or []
        pos_terms = info.get("pos_terms", []) or []
        neg_bonus = float(info.get("neg_bonus", 1.0))
        pos_bonus = float(info.get("pos_bonus", 1.0))
        if (neg_terms or pos_terms) and hasattr(vec, "vocabulary_"):
            vocab = vec.vocabulary_
            X_csc = X_csr.tocsc(copy=True)
            if neg_terms and neg_bonus != 1.0:
                X_csc = apply_column_scaling_csc(X_csc, vocab, neg_terms, neg_bonus)
            if pos_terms and pos_bonus != 1.0:
                X_csc = apply_column_scaling_csc(X_csc, vocab, pos_terms, pos_bonus)
            return X_csc.tocsr(copy=False)
    except Exception:
        return X_csr
    return X_csr

# ============= ëª¨ë¸ ë¡œë“œ (ìŠ¤í¬ë¦½íŠ¸ì™€ ë™ì¼ ì •ì±…) =============
@st.cache_resource(show_spinner=True)
def load_assets():
    # SGD (ì„¤ëª…ìš©) ìˆìœ¼ë©´ ë¡œë“œ
    cls = None
    if HAS_SKOPS and (MODELS / "sgd_logistic_cls.skops").exists():
        cls = safe_skops_load(MODELS / "sgd_logistic_cls.skops")
    elif (MODELS / "sgd_logistic_cls.joblib").exists():
        cls = joblib.load(MODELS / "sgd_logistic_cls.joblib")
    else:
        cls = None
    n_exp_cls = getattr(cls, "n_features_in_", None) if cls is not None else None

    # RF
    if HAS_SKOPS and (MODELS / "rf_reg.skops").exists():
        reg = safe_skops_load(MODELS / "rf_reg.skops")
    elif (MODELS / "rf_reg.joblib").exists():
        reg = joblib.load(MODELS / "rf_reg.joblib")
    else:
        raise FileNotFoundError("rf_reg.(skops|joblib) ì—†ìŒ")
    reg = _patch_rf_monotonic(reg)
    n_exp_reg = getattr(reg, "n_features_in_", None)

    # Vectorizer (cls ê¸°ì¤€ìœ¼ë¡œ ìš°ì„  ë§ì¶”ê³ , ë¶ˆì¼ì¹˜ ì‹œ ì—ëŸ¬)
    vec = load_vectorizer_with_compat(MODELS, n_exp_cls)
    n_vec = len(vec.get_feature_names_out())
    if (n_exp_reg is not None) and (n_vec != n_exp_reg):
        raise ValueError(f"ë²¡í„°ë¼ì´ì €({n_vec}) != RF ê¸°ëŒ€({n_exp_reg})")

    # stopwords
    stop = set()
    for sw in [MODELS.parent / "stopwords_ko.txt", MODELS / "stopwords_ko.txt"]:
        if Path(sw).exists():
            stop = {x.strip() for x in Path(sw).read_text(encoding="utf-8").splitlines() if x.strip()}
            break

    return vec, reg, cls, stop

vec, reg, sgd, stopwords = load_assets()

# ============= ìœ„í—˜ë„ =============
def risk_level(avg_score: float) -> str:
    if avg_score >= 4.10: return "Safe"
    if avg_score >= 4.00: return "Low"
    if avg_score >= 3.90: return "Medium"
    return "High"

def risk_color(level: str) -> str:
    return {"Safe": "#2e7d32","Low": "#558b2f","Medium": "#f9a825","High": "#c62828"}.get(level, "#333")

# ============= SGD ê¸°ì—¬ë„(ë¶€ì • TOP3 ì§‘ê³„) =============
def negative_top3_across_rows(X: csr_matrix, vec, sgd, tfidf_min=0.10, k=3):
    """
    5ì (ê°€ì¥ ê¸ì •) í´ë˜ìŠ¤ì˜ ê³„ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ contrib=coef*tfidfê°€ ìŒìˆ˜ì¸ í† í°ë“¤ë§Œ í•©ì‚°,
    ì „ì²´ í–‰ì— ëŒ€í•´ ëˆ„ì  í›„ ê°€ì¥ ë§ˆì´ë„ˆìŠ¤ê°€ í° 3ê°œ ë°˜í™˜.
    """
    if (sgd is None) or (not hasattr(sgd, "coef_")):
        return []
    feats = np.asarray(vec.get_feature_names_out())
    classes = getattr(sgd, "classes_", None)
    if classes is None:
        idx5 = sgd.coef_.shape[0]-1
    else:
        idx_arr = np.where(classes == 5)[0]
        idx5 = int(idx_arr[0]) if len(idx_arr) else sgd.coef_.shape[0]-1
    coef = sgd.coef_[idx5]
    contrib_sum = defaultdict(float)

    # í–‰ ë‹¨ìœ„ ëˆ„ì 
    for i in range(X.shape[0]):
        row = X[i].toarray().ravel()
        mask = (row >= tfidf_min)
        if not np.any(mask): 
            continue
        contrib = coef[mask] * row[mask]
        # ìŒìˆ˜ë§Œ
        neg_idx = np.where(contrib < 0)[0]
        for local_j in neg_idx:
            j = np.where(mask)[0][local_j]
            contrib_sum[int(j)] += float(contrib[local_j])

    if not contrib_sum:
        return []
    # ê°€ì¥ ë§ˆì´ë„ˆìŠ¤(ì‘ì€ ê°’) 3ê°œ
    items = sorted(contrib_sum.items(), key=lambda x: x[1])[:k]
    return [(feats[j], v) for j, v in items]

# ============= UI: ë‹¨ì¼ ì˜ˆì¸¡ =============
st.subheader("ë‹¨ì¼ í…ìŠ¤íŠ¸ ì˜ˆì¸¡")
colA, colB = st.columns([3,1])
with colA:
    inp = st.text_area("ë¦¬ë·° í…ìŠ¤íŠ¸ ì…ë ¥", height=140, placeholder="ë¦¬ë·°ë¥¼ ë¶™ì—¬ë„£ìœ¼ì„¸ìš”â€¦")
with colB:
    show_sgd = st.toggle("ì„¤ëª… ì‚¬ìš©(ë¶€ì • ë‹¨ì–´)", value=True, help="ë¶€ì • ë‹¨ì–´ TOP3ë¥¼ ê³„ì‚°")

if st.button("ì˜ˆì¸¡í•˜ê¸°") and inp.strip():
    toks = tokenize_and_join(inp, stopwords)
    X = vec.transform([toks])
    X = maybe_apply_negpos_bonus(X, vec, MODELS)
    score = float(np.clip(reg.predict(X)[0], 1, 5))
    st.metric("ì˜ˆì¸¡ ì ìˆ˜", f"{score:.2f} â˜…")

    if show_sgd:
        neg_top3 = negative_top3_across_rows(X, vec, sgd, tfidf_min=0.10, k=3)
        if neg_top3:
            parts = [f"{w} (Î£ {v:.2f})" for w, v in neg_top3]
            st.markdown(f"**ë¦¬ë·° ì† ë¶€ì •ì  ë‹¨ì–´ TOP3:** " + ", ".join(parts))

st.divider()

# ============= UI: ë°°ì¹˜ ì˜ˆì¸¡ =============
st.subheader("ë°°ì¹˜ ì˜ˆì¸¡ (CSV ì—…ë¡œë“œ)")
csv = st.file_uploader("CSV ì—…ë¡œë“œ (í•„ìˆ˜ ì»¬ëŸ¼: review_text)", type=["csv"])

if csv is not None:
    try:
        df = pd.read_csv(csv)
    except Exception as e:
        st.error(f"CSV ë¡œë”© ì‹¤íŒ¨: {e}")
    else:
        if "review_text" not in df.columns:
            st.error("CSVì— 'review_text' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        else:
            texts = df["review_text"].fillna("").astype(str).tolist()
            toks  = [tokenize_and_join(t, stopwords) for t in texts]
            X     = vec.transform(toks)
            X     = maybe_apply_negpos_bonus(X, vec, MODELS)

            pred  = np.clip(reg.predict(X), 1, 5)
            df_out = df.copy()
            df_out["ì˜ˆì¸¡ ë³„ì "] = np.round(pred, 2)

            # í‘œ
            view_cols = []
            if "review_text" in df_out.columns: view_cols.append("review_text")
            if "review_date" in df_out.columns: view_cols.append("review_date")
            view_cols.append("ì˜ˆì¸¡ ë³„ì ")
            st.dataframe(df_out.loc[:, view_cols].rename(columns={
                "review_text":"ë¦¬ë·°","review_date":"ë‚ ì§œ"
            }), use_container_width=True)

            # í‰ê· /ìœ„í—˜ë„
            avg = float(np.round(pred.mean(), 2))
            level = risk_level(avg)
            c1, c2 = st.columns([1,1])
            with c1:
                st.metric("í‰ê·  í‰ì ", f"{avg:.2f} â˜…")
            with c2:
                st.markdown(
                    f"""<div style="padding:10px 12px;border-radius:10px;background:{risk_color(level)};color:#fff;display:inline-block;font-weight:600;">ìœ„í—˜ë„: {level}</div>""",
                    unsafe_allow_html=True,
                )

            # ë¶€ì • TOP3 (High/Mediumì¼ ë•Œë§Œ)
            if level in ("High","Medium") and show_sgd:
                neg_top3 = negative_top3_across_rows(X, vec, sgd, tfidf_min=0.10, k=3)
                if neg_top3:
                    parts = [f"{w} (Î£ {v:.2f})" for w, v in neg_top3]
                    st.markdown(f"**ë¦¬ë·° ì† ë¶€ì •ì  ë‹¨ì–´ TOP3:** " + ", ".join(parts))

            # ë‹¤ìš´ë¡œë“œ
            st.download_button(
                "ê²°ê³¼ CSV ë‹¤ìš´ë¡œë“œ",
                df_out.to_csv(index=False, encoding="utf-8-sig"),
                file_name="predictions.csv",
                mime="text/csv",
            )
