# app.py â€” Streamlit ë¦¬ë·° ì˜ˆì¸¡( predict_safe.py ì™€ ë™ì¹˜ íŒŒì´í”„ë¼ì¸ )
# -*- coding: utf-8 -*-

from __future__ import annotations
import os, re, json
from pathlib import Path
import numpy as np
import pandas as pd
import streamlit as st
import joblib
from scipy.sparse import csr_matrix

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# (0) ì»¤ë„/ìŠ¤ë ˆë“œ ì•ˆì „ (predict_safe.pyì™€ ë™ì¼)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
os.environ.setdefault("OMP_NUM_THREADS", "1")
os.environ.setdefault("OPENBLAS_NUM_THREADS", "1")
os.environ.setdefault("MKL_NUM_THREADS", "1")
os.environ.setdefault("NUMEXPR_NUM_THREADS", "1")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ê¸°ë³¸ ê²½ë¡œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="ë¦¬ë·° ì˜ˆì¸¡/ë¶„ì„", page_icon="â­", layout="wide")
ROOT   = Path(__file__).resolve().parent
MODELS = ROOT / "models"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# skops ë¡œë” (ìˆìœ¼ë©´ ì‚¬ìš©) + ì•ˆì „ ë¡œë“œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    from skops.io import load as skops_load, get_untrusted_types
    HAS_SKOPS = True
except Exception:
    HAS_SKOPS = False

ALLOWED_PREFIXES = ("sklearn.", "numpy.", "scipy.", "xgboost.", "lightgbm.")

def safe_skops_load(path: Path):
    p = str(path)
    try:
        types = get_untrusted_types(file=p)
    except TypeError:
        try:
            types = get_untrusted_types(path=p)
        except TypeError:
            types = get_untrusted_types()
    bad = [t for t in types if not t.startswith(ALLOWED_PREFIXES)]
    if bad:
        raise RuntimeError(
            "skops íŒŒì¼ì— ë¹„í—ˆìš© íƒ€ì…ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n"
            f"- íŒŒì¼: {p}\n- ë¹„í—ˆìš© ì˜ˆ: {bad[:5]} ..."
        )
    return skops_load(p, trusted=types)

def _patch_rf_monotonic(reg):
    # RandomForestRegressor í•˜ìœ„ íŠ¸ë¦¬ monotonic_cst ëˆ„ë½ ë³´ì •
    try:
        from sklearn.ensemble import RandomForestRegressor
        from sklearn.pipeline import Pipeline
        rf = None
        if hasattr(reg, "steps"):  # Pipeline
            last = reg.steps[-1][1]
            if isinstance(last, RandomForestRegressor):
                rf = last
        elif isinstance(reg, RandomForestRegressor):
            rf = reg
        if rf is not None and getattr(rf, "estimators_", None) is not None:
            for est in rf.estimators_:
                if not hasattr(est, "monotonic_cst"):
                    setattr(est, "monotonic_cst", None)
    except Exception:
        pass
    return reg

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì „ì²˜ë¦¬/í† í¬ë‚˜ì´ì¦ˆ (predict_safe.pyì™€ ë™ì¼)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
POS_EMO = "ğŸ˜€ğŸ˜ƒğŸ˜„ğŸ˜ğŸ˜†ğŸ™‚ğŸ˜ŠğŸ˜ğŸ¤©ğŸ˜‹ğŸ˜‰ğŸ‘ğŸ™ŒğŸ‰â¤ğŸ’–ğŸ’—ğŸ’“ğŸ’ğŸ’•âœ¨ğŸ˜»ğŸ¥°ğŸ¤—ğŸ˜ºğŸ˜¸"
NEG_EMO = "ğŸ˜ğŸ˜ŸğŸ˜ ğŸ˜¡ğŸ˜¢ğŸ˜­ğŸ¤®ğŸ˜’ğŸ˜•ğŸ™â˜¹ğŸ‘ğŸ’¢ğŸ˜£ğŸ˜–ğŸ¤¬ğŸ˜¤ğŸ’”ğŸ˜¿ğŸ˜¹"
URL_RE = re.compile(r"(https?:\/\/[^\s]+)")
HTML_RE = re.compile(r"<[^>]+>")
MULTI_SPACE = re.compile(r"\s+")

def replace_emojis(text: str) -> str:
    text = re.sub(f"[{re.escape(POS_EMO)}]+", " [EMO_POS] ", text)
    text = re.sub(f"[{re.escape(NEG_EMO)}]+", " [EMO_NEG] ", text)
    text = re.sub(r"[\U00010000-\U0010ffff]", " [EMO] ", text)
    return text

def clean_text(s: str) -> str:
    if not isinstance(s, str):
        return ""
    s = s.replace("\u200b", " ")
    s = HTML_RE.sub(" ", s)
    s = URL_RE.sub(" [URL] ", s)
    s = replace_emojis(s)
    s = re.sub(r"[^0-9A-Za-zê°€-í£\.\,\!\?\[\]_ ]+", " ", s)
    s = MULTI_SPACE.sub(" ", s).strip()
    return s

def get_tokenizer():
    # 1) mecab
    try:
        from mecab import MeCab
        m = MeCab()
        def tok(text):
            return [w for (w, p) in m.pos(text)
                    if p.startswith(("NN","VV","VA","MAG","IC","XR"))]
        return "mecab_python", tok
    except Exception:
        pass
    # 2) kiwi
    try:
        from kiwipiepy import Kiwi
        kiwi = Kiwi()
        def tok(text):
            return [t.form for t in kiwi.tokenize(text)
                    if t.tag.startswith(("N","V","MAG","IC","XR","MM"))]
        return "kiwi", tok
    except Exception:
        pass
    # 3) fallback: regex
    def tok(text):
        return re.findall(r"[ê°€-í£A-Za-z0-9]{2,}", text)
    return "simple", tok

TOK_NAME, TOKENIZE = get_tokenizer()

@st.cache_data(show_spinner=False)
def load_stopwords() -> set[str]:
    # stopwords_ko.txtê°€ models/ ë˜ëŠ” ìƒìœ„ì— ìˆìœ¼ë©´ ë¶ˆëŸ¬ì˜¤ê¸°
    for sw in [ROOT / "stopwords_ko.txt", MODELS / "stopwords_ko.txt", ROOT.parent / "stopwords_ko.txt"]:
        if sw.exists():
            with open(sw, encoding="utf-8") as f:
                return {x.strip() for x in f if x.strip()}
    return set()

def tokenize_and_join(s: str, stop:set[str]) -> str:
    toks = TOKENIZE(clean_text(s))
    if stop:
        toks = [t for t in toks if t not in stop]
    return " ".join(toks)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# neg/pos ì—´ê°€ì¤‘ (predict_safe.py ë™ì¼)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def apply_column_scaling_csc(X_csc, vocab: dict, terms: list, scale: float):
    hits = [vocab[t] for t in terms if t in vocab]
    for j in hits:
        start, end = X_csc.indptr[j], X_csc.indptr[j+1]
        if end > start:
            X_csc.data[start:end] *= scale
    return X_csc

def maybe_apply_negpos_bonus(X_csr, vec, base_dir: Path):
    summ = base_dir / "tfidf_summary.json"
    if not summ.exists():
        return X_csr
    try:
        with open(summ, "r", encoding="utf-8") as f:
            js = json.load(f)
        info = js.get("tfidf", {})
        neg_terms = info.get("neg_terms", []) or []
        pos_terms = info.get("pos_terms", []) or []
        neg_bonus = float(info.get("neg_bonus", 1.0))
        pos_bonus = float(info.get("pos_bonus", 1.0))
        if (neg_terms or pos_terms) and hasattr(vec, "vocabulary_"):
            vocab = vec.vocabulary_
            X_csc = X_csr.tocsc(copy=True)
            if neg_terms and neg_bonus != 1.0:
                X_csc = apply_column_scaling_csc(X_csc, vocab, neg_terms, neg_bonus)
            if pos_terms and pos_bonus != 1.0:
                X_csc = apply_column_scaling_csc(X_csc, vocab, pos_terms, pos_bonus)
            return X_csc.tocsr(copy=False)
    except Exception:
        return X_csr
    return X_csr

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë²¡í„°ë¼ì´ì €/ëª¨ë¸ ë¡œë“œ (predict_safe.pyì™€ ë™ì¼í•œ ìš°ì„ ìˆœìœ„)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_vectorizer_with_compat(base: Path, model_expected_n: int|None):
    cands = []
    sk = base / "tfidf_vectorizer.skops"
    pk = base / "tfidf_vectorizer.pkl"
    if sk.exists() and HAS_SKOPS: cands.append(("skops", sk))
    if pk.exists():               cands.append(("pkl",   pk))
    if not cands:
        raise FileNotFoundError("tfidf_vectorizer.(skops|pkl) ê°€ ì—†ìŠµë‹ˆë‹¤.")
    last_err, chosen = None, None
    for kind, path in cands:
        try:
            vec = safe_skops_load(path) if kind=="skops" else joblib.load(path)
            n_vec = len(vec.get_feature_names_out())
            if (model_expected_n is None) or (n_vec == model_expected_n):
                chosen = vec; break
            else:
                last_err = f"'{path.name}' has {n_vec} features, model expects {model_expected_n}"
        except Exception as e:
            last_err = f"load fail {path.name}: {e!r}"
    if chosen is None:
        raise ValueError(f"ì í•©í•œ TF-IDF ë²¡í„°ë¼ì´ì €ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë§ˆì§€ë§‰ ì˜¤ë¥˜: {last_err}")
    return chosen

@st.cache_resource(show_spinner=True)
def load_assets():
    base = MODELS
    # 1) sgd classifier (ì„¤ëª…/ë¶€ì • í† í° ì¶”ì¶œìš©)
    if HAS_SKOPS and (base / "sgd_logistic_cls.skops").exists():
        cls = safe_skops_load(base / "sgd_logistic_cls.skops")
    elif (base / "sgd_logistic_cls.joblib").exists():
        cls = joblib.load(base / "sgd_logistic_cls.joblib")
    else:
        st.error("sgd_logistic_cls.(skops|joblib) íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        st.stop()
    model_expected_n_cls = getattr(cls, "n_features_in_", None)

    # 2) rf regressor
    if HAS_SKOPS and (base / "rf_reg.skops").exists():
        reg = safe_skops_load(base / "rf_reg.skops")
    elif (base / "rf_reg.joblib").exists():
        reg = joblib.load(base / "rf_reg.joblib")
    else:
        st.error("rf_reg.(skops|joblib) íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        st.stop()
    reg = _patch_rf_monotonic(reg)
    model_expected_n_reg = getattr(reg, "n_features_in_", None)

    # 3) vectorizer (ë¶„ë¥˜ ê¸°ì¤€ìœ¼ë¡œ ë§ì¶”ê³ , íšŒê·€ì™€ë„ ì¼ì¹˜ í™•ì¸)
    vec = load_vectorizer_with_compat(base, model_expected_n_cls)
    n_vec = len(vec.get_feature_names_out())
    if (model_expected_n_reg is not None) and (n_vec != model_expected_n_reg):
        st.error(
            "[íŠ¹ì§• ë¶ˆì¼ì¹˜] ë¶„ë¥˜ ëª¨ë¸ê³¼ ë§ëŠ” ë²¡í„°ë¼ì´ì €ë¥¼ ì°¾ì•˜ì§€ë§Œ, íšŒê·€ ëª¨ë¸ê³¼ëŠ” í”¼ì²˜ ìˆ˜ê°€ ë‹¤ë¦…ë‹ˆë‹¤.\n"
            f"- vectorizer: {n_vec}, rf_reg expects: {model_expected_n_reg}"
        )
        st.stop()

    # stopwords(optional)
    stop = load_stopwords()
    return vec, cls, reg, stop, base

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìœ„í—˜ë„/ìƒ‰ìƒ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def risk_level(avg_score: float) -> str:
    if avg_score >= 4.10: return "Safe"
    if avg_score >= 4.00: return "Low"
    if avg_score >= 3.90: return "Medium"
    return "High"

def risk_color(level: str) -> str:
    return {"Safe":"#2e7d32","Low":"#558b2f","Medium":"#f9a825","High":"#c62828"}.get(level,"#333")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë¶€ì • ê¸°ì—¬ í† í° TOP-K (ë©€í‹°ë¼ì¸ ì „ì²´ ì§‘ê³„)
#  - ê° ë¦¬ë·°ë³„ë¡œ SGDê°€ ì˜ˆì¸¡í•œ classì˜ coefë¥¼ ì‚¬ìš©í•´ X.multiply(coef) í›„ ì—´ ë°©í–¥ í•©
#  - í•©ê³„ê°€ ê°€ì¥ ìŒìˆ˜(ë¶€ì •ì )ì¸ í”¼ì²˜ ìƒìœ„ K ë°˜í™˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def negative_topk_tokens(vec, cls, X:csr_matrix, k=3):
    feats = vec.get_feature_names_out()
    classes = getattr(cls, "classes_", None)
    y = cls.predict(X)
    totals = np.zeros(X.shape[1], dtype=np.float64)

    # í´ë˜ìŠ¤ë³„ë¡œ ë¬¶ì–´ ë²¡í„°í™”ëœ ê³± ìˆ˜í–‰ (ë¹ ë¦„)
    for c in np.unique(y):
        idx = np.where(y == c)[0]
        if idx.size == 0:
            continue
        Xsub = X[idx]
        if classes is None:
            y_idx = int(c) - 1
        else:
            pos = np.where(classes == c)[0]
            y_idx = int(pos[0]) if len(pos) else int(c) - 1
        coef = np.asarray(cls.coef_[y_idx]).ravel()  # (n_features,)
        # ìš”ì†Œê³± í›„ ì—´í•© (sparse-friendly)
        contrib_sum = Xsub.multiply(coef).sum(axis=0).A1  # shape (n_features,)
        totals += contrib_sum

    # ê°€ì¥ ìŒìˆ˜ì¸ í•­ëª© kê°œ
    neg_idx = np.argsort(totals)[:k]
    out = []
    for j in neg_idx:
        if totals[j] >= 0:
            break
        out.append((feats[j], float(totals[j])))
    return out  # [(token, total_contrib),...]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# UI
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.title("â­ ë¦¬ë·° ë³„ì  ì˜ˆì¸¡)")

vec, cls, reg, stop, base = load_assets()

st.caption(f"tokenizer = **{TOK_NAME}**, stopwords = {len(stop)}ê°œ")

# â”€â”€ ë‹¨ì¼ ì˜ˆì¸¡
st.subheader("ë‹¨ì¼ í…ìŠ¤íŠ¸ ì˜ˆì¸¡")
inp = st.text_area("ë¦¬ë·° í…ìŠ¤íŠ¸ ì…ë ¥", height=160, placeholder="ë¦¬ë·°ë¥¼ ë¶™ì—¬ë„£ìœ¼ì„¸ìš”â€¦")
if st.button("ì˜ˆì¸¡í•˜ê¸°") and inp.strip():
    toks = tokenize_and_join(inp, stop)
    X = vec.transform([toks])
    X = maybe_apply_negpos_bonus(X, vec, base)
    score = float(np.clip(reg.predict(X)[0], 1, 5))
    st.metric("ì˜ˆì¸¡ ë³„ì ", f"{score:.2f} â˜…")

st.divider()

# â”€â”€ ë°°ì¹˜ ì˜ˆì¸¡
st.subheader("ë°°ì¹˜ ì˜ˆì¸¡ (CSV ì—…ë¡œë“œ)")
csv = st.file_uploader("CSV ì—…ë¡œë“œ (í•„ìˆ˜ ì»¬ëŸ¼: review_text)", type=["csv"])

if csv is not None:
    try:
        df = pd.read_csv(csv)
    except Exception as e:
        st.error(f"CSV ë¡œë”© ì‹¤íŒ¨: {e}")
    else:
        if "review_text" not in df.columns:
            st.error("CSVì— 'review_text' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        else:
            texts = df["review_text"].fillna("").astype(str).tolist()
            toks  = [tokenize_and_join(t, stop) for t in texts]
            X = vec.transform(toks)
            X = maybe_apply_negpos_bonus(X, vec, base)

            # ì˜ˆì¸¡ (predict_safe.py ì™€ ë™ì¼: RF íšŒê·€ ì ìˆ˜ â†’ 1~5 í´ë¦½)
            pred = np.clip(reg.predict(X), 1, 5).round(2)

            # í™”ë©´ í‘œì‹œìš©
            view_cols = []
            if "review_text" in df.columns: view_cols.append("review_text")
            if "review_date" in df.columns: view_cols.append("review_date")
            view_cols.append("pred_score")

            out = df.copy()
            out["pred_score"] = pred
            out_view = out.loc[:, view_cols].rename(columns={
                "review_text":"ë¦¬ë·°", "review_date":"ë‚ ì§œ", "pred_score":"ì˜ˆì¸¡ ë³„ì "
            })
            st.dataframe(out_view, use_container_width=True)

            # í‰ê·  & ìœ„í—˜ë„
            avg = float(out["pred_score"].mean())
            level = risk_level(avg)

            c1, c2 = st.columns([1,1])
            with c1:
                st.metric("í‰ê·  í‰ì ", f"{avg:.2f} â˜…")
            with c2:
                st.markdown(
                    f"""<div style="padding:10px 12px;border-radius:10px;
                                 background:{risk_color(level)};color:#fff;
                                 display:inline-block;font-weight:600;">
                         ìœ„í—˜ë„: {level}
                        </div>""",
                    unsafe_allow_html=True
                )

            # ìœ„í—˜ë„ê°€ Medium/High ì´ë©´ ë¶€ì •ì  ê¸°ì—¬ í† í° TOP3
            if level in ("Medium", "High"):
                neg_top3 = negative_topk_tokens(vec, cls, X, k=3)
                if neg_top3:
                    pretty = " Â· ".join([f"{tok} (âˆ‘ {val:.2f})" for tok, val in neg_top3])
                    st.markdown(
                        f"**ë¦¬ë·° ì† ë¶€ì •ì  ë‹¨ì–´ TOP3**: {pretty}"
                    )

            # ë‹¤ìš´ë¡œë“œ
            st.download_button(
                "ê²°ê³¼ CSV ë‹¤ìš´ë¡œë“œ",
                out.to_csv(index=False, encoding="utf-8-sig"),
                file_name="predictions.csv",
                mime="text/csv",
            )
